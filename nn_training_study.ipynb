{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c83964",
   "metadata": {},
   "source": [
    "# Cesar\n",
    "# David\n",
    "# Ivanna\n",
    "# Maximiliano\n",
    "\n",
    "\n",
    "\n",
    "# Laboratorio 2 – Entrenamiento de Redes Neuronales\n",
    "\n",
    "## **Introducción**\n",
    "\n",
    "En este laboratorio se desarrollará un estudio sistemático sobre el **entrenamiento de redes neuronales convolucionales (CNNs)** utilizando **PyTorch**.  \n",
    "El objetivo principal es comprender cómo diferentes decisiones de diseño afectan el comportamiento del entrenamiento, la capacidad de generalización y la estabilidad del modelo.\n",
    "\n",
    "A lo largo del proyecto se implementará un **pipeline de clasificación de imágenes**, desde la preparación del dataset hasta la evaluación final del modelo.  \n",
    "A partir de una configuración base (*baseline*), se realizarán **experimentos controlados (ablaciones)** para analizar el impacto de cinco factores clave en el rendimiento:\n",
    "\n",
    "1. **Optimizadores:** comparación entre métodos como *SGD*, *Adam* y *AdamW*.  \n",
    "2. **Regularización:** estudio de técnicas como *weight decay*, *dropout* y *label smoothing*.  \n",
    "3. **Tamaño de batch:** análisis de su influencia en la convergencia y la generalización.  \n",
    "4. **Esquemas de tasa de aprendizaje (LR Schedules):** evaluación de estrategias como *cosine annealing*, *step decay* y *OneCycle*.  \n",
    "5. **Inicialización de pesos:** comparación entre métodos como *Kaiming (He)*, *Xavier (Glorot)* y *Orthogonal*.\n",
    "\n",
    "El conjunto de datos será construido mediante **web scraping**, asegurando al menos **10 clases** con un mínimo de **60 imágenes por clase**.  \n",
    "Posteriormente, se aplicarán **técnicas de aumento de datos (data augmentation)** para incrementar el conjunto hasta aproximadamente **6,000 imágenes**.\n",
    "\n",
    "Cada experimento se ejecutará bajo condiciones **reproducibles** (semillas fijas, divisiones de datos constantes y control del presupuesto computacional).  \n",
    "Los resultados incluirán **curvas de pérdida y precisión**, **tablas comparativas (media ± desviación estándar)** y **análisis de tendencias** para cada eje experimental.\n",
    "\n",
    "Finalmente, se discutirán las observaciones más relevantes y se conectarán los resultados con los fundamentos teóricos sobre **optimización, generalización y dinámica del entrenamiento** en redes neuronales profundas.\n",
    "\n",
    "---\n",
    "\n",
    "## **Objetivos del Laboratorio**\n",
    "\n",
    "- Implementar un **pipeline completo de entrenamiento** en PyTorch con control de reproducibilidad, métricas y checkpoints.  \n",
    "- Comprender las **implicaciones prácticas y teóricas** de los diferentes optimizadores, métodos de regularización, esquemas de tasa de aprendizaje y estrategias de inicialización.  \n",
    "- Evaluar la influencia del **tamaño de batch** en la estabilidad del entrenamiento y la capacidad de generalización del modelo.  \n",
    "- Diseñar y ejecutar **experimentos de ablasión controlados**, variando un solo factor a la vez para analizar su impacto de forma aislada.  \n",
    "- Analizar los resultados mediante **gráficas de pérdida y precisión**, e interpretar los efectos observados en términos de **teoría del aprendizaje profundo**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad71f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.version.cuda)\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf36c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import DataLoader, Subset \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import math, numpy as np, random, matplotlib.pyplot as plt\n",
    "import time, os, copy, logging\n",
    "from tqdm import tqdm\n",
    "from torch_lr_finder import LRFinder\n",
    "import math, random, copy\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "import csv, json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a5145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RAW_DIR = Path(\"DatasetCNN\")\n",
    "\n",
    "\n",
    "from torchvision import datasets\n",
    "base_ds = datasets.ImageFolder(root=RAW_DIR)\n",
    "print(\"Clases detectadas:\", base_ds.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eb747",
   "metadata": {},
   "source": [
    "En este fragmento se **define la ruta base** donde se encuentra el conjunto de datos de imágenes (`RAW_DIR`) y se **carga el dataset** utilizando `ImageFolder` de `torchvision`.\n",
    "\n",
    "Este método permite **estructurar automáticamente las clases** a partir de las carpetas del directorio raíz, lo que facilita la lectura y organización del dataset para su posterior procesamiento y entrenamiento del modelo.\n",
    "\n",
    "Finalmente, se imprime la **lista de clases detectadas**, confirmando que las categorías fueron reconocidas correctamente desde la estructura de carpetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342020ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ.pop(\"CUDA_LAUNCH_BLOCKING\", None)\n",
    "\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f84d0",
   "metadata": {},
   "source": [
    "En este bloque se realiza la **configuración inicial del entorno de ejecución** para asegurar un comportamiento controlado y eficiente durante el entrenamiento:\n",
    "\n",
    "- Se elimina la variable `CUDA_LAUNCH_BLOCKING` (si existe) para evitar bloqueos innecesarios en la ejecución GPU.  \n",
    "- Se define la función `set_seed()` para **establecer una semilla fija** en las librerías `random`, `numpy` y `torch`, garantizando la **reproducibilidad** de los resultados.  \n",
    "- Se configuran parámetros de **CuDNN**:\n",
    "  - `deterministic = False` permite cierto grado de aleatoriedad para mejorar el rendimiento.\n",
    "  - `benchmark = True` activa una búsqueda automática del mejor algoritmo de convolución según el hardware disponible.  \n",
    "- Finalmente, se define el **dispositivo de cómputo (`device`)**, seleccionando **GPU (cuda)** si está disponible, o **CPU** en caso contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f6294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = r\"C:\\Users\\cesar\\Downloads\\DatasetCNN\"  \n",
    "\n",
    "tmp = datasets.ImageFolder(RAW_DIR)  # solo para leer clases\n",
    "classes = tmp.classes\n",
    "label_dict = {i: c for i, c in enumerate(classes)}\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727825d2",
   "metadata": {},
   "source": [
    "En este bloque se **lee nuevamente el dataset** con `ImageFolder` para **obtener las clases disponibles** en la carpeta raíz.  \n",
    "A partir de esa información se construye un **diccionario de etiquetas (`label_dict`)**, donde cada clase se asocia a un número entero, facilitando su uso dentro del modelo.\n",
    "\n",
    "El resultado muestra las **10 categorías del conjunto de datos**, cada una representada con su respectivo índice numérico:\n",
    "\n",
    "\n",
    "Este mapeo se utilizará más adelante para **interpretar las predicciones del modelo** y para el manejo de etiquetas durante el entrenamiento y la evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf2c46",
   "metadata": {},
   "source": [
    "En este fragmento se definen las **transformaciones de preprocesamiento y aumento de datos** que se aplicarán a las imágenes del conjunto de entrenamiento y de prueba:\n",
    "\n",
    "- `IMG_SIZE = 224` establece el **tamaño de entrada** de las imágenes, compatible con arquitecturas comunes como *AlexNet* o *VGG*.  \n",
    "- Se definen las constantes `IMAGENET_MEAN` y `IMAGENET_STD`, utilizadas para **normalizar los valores de píxel** según las estadísticas del conjunto *ImageNet*, mejorando la estabilidad del entrenamiento.\n",
    "\n",
    "### **Transformaciones para entrenamiento (`train_tf`):**\n",
    "Incluyen operaciones de **aumento de datos (data augmentation)** que ayudan a mejorar la generalización del modelo:\n",
    "- `RandomResizedCrop`: recorta y reescala aleatoriamente partes de la imagen.  \n",
    "- `RandomHorizontalFlip`: invierte la imagen horizontalmente de forma aleatoria.  \n",
    "- `ToTensor` y `Normalize`: convierten la imagen a tensor y normalizan sus valores.\n",
    "\n",
    "### **Transformaciones para validación/prueba (`test_tf`):**\n",
    "Se aplican transformaciones determinísticas, sin aleatoriedad:\n",
    "- `Resize` y `CenterCrop`: ajustan las imágenes al tamaño esperado.  \n",
    "- `ToTensor` y `Normalize`: preparan los datos en el mismo formato que el conjunto de entrenamiento.\n",
    "\n",
    "Estas transformaciones garantizan que las imágenes estén **preparadas, escaladas y normalizadas** correctamente antes de ser utilizadas por la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_indices(dataset, train_ratio=0.8, seed=2025):\n",
    "    set_seed(seed)\n",
    "    targets = np.array([y for _, y in dataset.samples])  # (path, class_idx)\n",
    "    class_ids = np.unique(targets)\n",
    "\n",
    "    idx_train, idx_test = [], []\n",
    "    for c in class_ids:\n",
    "        idx = np.where(targets == c)[0].tolist()\n",
    "        random.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        n_train = int(n * train_ratio)\n",
    "        idx_train += idx[:n_train]\n",
    "        idx_test  += idx[n_train:]\n",
    "    random.shuffle(idx_train); random.shuffle(idx_test)\n",
    "    return idx_train, idx_test\n",
    "\n",
    "# dataset base sin transforms solo para partir índices\n",
    "base_ds = datasets.ImageFolder(RAW_DIR, transform=None)\n",
    "idx_tr, idx_te = stratified_train_test_indices(base_ds, train_ratio=0.80, seed=2025)\n",
    "\n",
    "len(idx_tr), len(idx_te), len(base_ds.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a1538e",
   "metadata": {},
   "source": [
    "En este bloque se construye una **división estratificada** del dataset en subconjuntos de entrenamiento y validación/prueba.\n",
    "\n",
    "1. La función `stratified_train_test_indices`:\n",
    "   - Recibe el dataset completo y un `train_ratio` (por defecto 80% entrenamiento, 20% validación/prueba).\n",
    "   - Agrupa los índices por clase y los **baraja con una semilla fija** para mantener reproducibilidad.\n",
    "   - Asigna proporcionalmente una parte de cada clase al conjunto de entrenamiento y el resto al conjunto de validación/prueba.\n",
    "   - Devuelve dos listas: `idx_train` e `idx_test`, que contienen los índices seleccionados.\n",
    "\n",
    "2. Se crea `base_ds` usando `ImageFolder` sin transforms. Esto se usa únicamente para **calcular los índices** de cada split, sin modificar todavía las imágenes.\n",
    "\n",
    "3. Se llama a la función para obtener:\n",
    "   - `idx_tr`: índices para entrenamiento.\n",
    "   - `idx_te`: índices para validación/prueba.\n",
    "\n",
    "4. Finalmente, se imprime:\n",
    "   - El tamaño del conjunto de entrenamiento (`730` imágenes),\n",
    "   - El tamaño del conjunto de validación/prueba (`188` imágenes),\n",
    "   - Y el número de clases (`10` clases).\n",
    "\n",
    "Con esto aseguramos que **todas las clases están representadas en ambas particiones** y que la proporción 80/20 se respeta de forma balanceada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91018644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "train_ds = Subset(datasets.ImageFolder(RAW_DIR, transform=train_tf), idx_tr)\n",
    "test_ds  = Subset(datasets.ImageFolder(RAW_DIR, transform=test_tf),  idx_te)\n",
    "\n",
    "BATCH = 128  \n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "num_classes = len(base_ds.classes)\n",
    "num_classes, classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48fd3cc",
   "metadata": {},
   "source": [
    "En este bloque se preparan los **datasets y dataloaders** que se usarán para entrenar y evaluar la red neuronal.\n",
    "\n",
    "1. Se importan utilidades de **PIL** y se configuran parámetros para **evitar errores al cargar imágenes incompletas o muy grandes**, asegurando una lectura estable del dataset.\n",
    "\n",
    "2. Se crean los subconjuntos:\n",
    "   - `train_ds`: dataset de entrenamiento con las transformaciones `train_tf`.\n",
    "   - `test_ds`: dataset de validación/prueba con las transformaciones `test_tf`.  \n",
    "   Ambos se generan usando `Subset`, que selecciona las imágenes correspondientes a los índices obtenidos previamente.\n",
    "\n",
    "3. Se definen parámetros para el cargador de datos (`DataLoader`):\n",
    "   - `BATCH = 128`: número de imágenes procesadas por iteración.  \n",
    "   - `NUM_WORKERS = 4`: cantidad de procesos paralelos para cargar datos.  \n",
    "   - `PIN_MEMORY = True`: optimiza el paso de datos a la GPU.  \n",
    "\n",
    "4. Se crean los **dataloaders**:\n",
    "   - `train_dl`: baraja los datos en cada época (`shuffle=True`) para mejorar la generalización.  \n",
    "   - `test_dl`: no baraja los datos (`shuffle=False`) ya que se usa solo para evaluación.\n",
    "\n",
    "5. Finalmente, se obtiene el número de clases (`num_classes`) y la lista con los nombres de cada clase (`classes`), información que será necesaria para **definir la capa de salida del modelo** y **mostrar resultados interpretables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a45548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util para des-normalizar\n",
    "import torch\n",
    "\n",
    "def denormalize(img_tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    mean = torch.tensor(mean).view(3,1,1)\n",
    "    std  = torch.tensor(std).view(3,1,1)\n",
    "    return (img_tensor * std) + mean\n",
    "\n",
    "def show_samples(dataset, n=5, title=\"Preview\"):\n",
    "    # samplea índices aleatorios del dataset\n",
    "    idxs = random.sample(range(len(dataset)), k=min(n, len(dataset)))\n",
    "    # recoge (img, label) aplicando el transform del dataset\n",
    "    samples = [dataset[i] for i in idxs]\n",
    "    imgs = [s[0] for s in samples]\n",
    "    labels = [s[1] for s in samples]\n",
    "\n",
    "    rows = 2; cols = (n + rows - 1)//rows\n",
    "    plt.figure(figsize=(2.5*cols, 2.8*rows))\n",
    "    for i, (img, lab) in enumerate(zip(imgs, labels)):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        img_dn = denormalize(img).clamp(0,1)     # [0,1] para mostrar\n",
    "        plt.imshow(img_dn.permute(1,2,0).cpu())\n",
    "        plt.title(label_dict[lab], fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Previews (puedes usar train_ds o test_ds)\n",
    "show_samples(Subset(datasets.ImageFolder(RAW_DIR, transform=test_tf), idx_te), n=10, title=\"Test preview\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc1e2d",
   "metadata": {},
   "source": [
    "En este bloque se implementan funciones auxiliares para **visualizar imágenes del dataset** y comprobar que las transformaciones se aplicaron correctamente.\n",
    "\n",
    "1. **Función `denormalize()`:**  \n",
    "   - Revierte la normalización aplicada con las medias y desviaciones estándar de *ImageNet*.  \n",
    "   - Permite que las imágenes vuelvan a un rango de colores visible `[0,1]` para poder mostrarlas con `matplotlib`.\n",
    "\n",
    "2. **Función `show_samples()`:**  \n",
    "   - Selecciona aleatoriamente un conjunto de imágenes del dataset.  \n",
    "   - Aplica el *transform* correspondiente (entrenamiento o prueba).  \n",
    "   - Desnormaliza las imágenes y las muestra en una cuadrícula junto con el nombre de su clase.  \n",
    "   - Se utiliza `label_dict` para traducir las etiquetas numéricas a nombres legibles.\n",
    "\n",
    "3. Finalmente, se llama a `show_samples()` sobre una muestra del conjunto de **prueba (`test_ds`)** para generar una **vista previa de las imágenes** y confirmar que la estructura del dataset, las clases y los *transforms* fueron definidos correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion de las imagenes y adapatdas para el uso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "SRC = Path(r\"C:\\Users\\cesar\\Downloads\\DatasetCNN\")  \n",
    "DST = Path(r\"C:\\Datasets\\DatasetCNN_256\")            \n",
    "DST.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "count_in, count_out, bad = 0, 0, []\n",
    "for cls_dir in SRC.iterdir():\n",
    "    if not cls_dir.is_dir(): \n",
    "        continue\n",
    "    out_dir = DST/cls_dir.name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for p in cls_dir.iterdir():\n",
    "        if not p.is_file(): \n",
    "            continue\n",
    "        try:\n",
    "            im = Image.open(p).convert(\"RGB\")\n",
    "            im = im.resize((256, 256), Image.BILINEAR)\n",
    "            (out_dir/(p.stem + \".jpg\")).write_bytes(b\"\")  # reserva\n",
    "            im.save(out_dir/(p.stem + \".jpg\"), format=\"JPEG\", quality=90, optimize=True)\n",
    "            count_out += 1\n",
    "        except Exception as e:\n",
    "            bad.append((str(p), repr(e)))\n",
    "        count_in += 1\n",
    "\n",
    "print(f\"Procesadas OK: {count_out}/{count_in}  | Carpeta destino: {DST}\")\n",
    "if bad:\n",
    "    print(\"Archivos problemáticos:\", len(bad))\n",
    "    print(bad[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35cca2a",
   "metadata": {},
   "source": [
    "En este bloque se realiza una **preparación y limpieza del conjunto de datos**, asegurando que todas las imágenes tengan un formato y tamaño uniforme antes de ser utilizadas en el entrenamiento.\n",
    "\n",
    "1. **Configuración inicial:**\n",
    "   - Se importan utilidades de `PIL` y se activan opciones para **permitir la carga de imágenes truncadas o muy grandes** sin errores.\n",
    "   - Se definen las rutas de origen (`SRC`) y destino (`DST`), donde se guardará la versión procesada del dataset.\n",
    "   - Si la carpeta destino no existe, se **crea automáticamente** con `mkdir()`.\n",
    "\n",
    "2. **Proceso de redimensionamiento y guardado:**\n",
    "   - Se recorren las subcarpetas del dataset original, una por cada clase.  \n",
    "   - Cada imagen se abre, se **convierte a formato RGB**, y se **redimensiona a 256×256 píxeles** usando interpolación bilineal.  \n",
    "   - Luego, se guarda en formato **JPEG optimizado** en la carpeta correspondiente dentro del nuevo directorio destino.\n",
    "\n",
    "3. **Control y validación:**\n",
    "   - Se lleva un conteo de imágenes procesadas correctamente (`count_out`) y del total revisado (`count_in`).  \n",
    "   - Si alguna imagen genera un error, se agrega a la lista `bad` para su revisión posterior.  \n",
    "   - Al finalizar, se imprime un resumen con el número total de imágenes procesadas exitosamente y la ubicación del nuevo dataset.\n",
    "\n",
    "Este procedimiento garantiza que **todas las imágenes del dataset estén limpias, homogéneas y listas** para ser utilizadas en las etapas de entrenamiento y validación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = str(DST)  # usa el cache 256\n",
    "\n",
    "# Clases\n",
    "base = datasets.ImageFolder(RAW_DIR)\n",
    "classes = base.classes\n",
    "num_classes = len(classes)\n",
    "print(\"Clases:\", classes)\n",
    "\n",
    "# Transforms (ligeros, rápidos)\n",
    "IMG_SIZE = 224\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# 80/20 estratificado\n",
    "def stratified_train_test_indices(dataset, train_ratio=0.8, seed=2025):\n",
    "    import numpy as np, random\n",
    "    random.seed(seed)\n",
    "    targets = np.array([y for _, y in dataset.samples])\n",
    "    cls_ids = np.unique(targets)\n",
    "    idx_tr, idx_te = [], []\n",
    "    for c in cls_ids:\n",
    "        idx = np.where(targets==c)[0].tolist()\n",
    "        random.shuffle(idx)\n",
    "        n = len(idx); n_tr = int(n*train_ratio)\n",
    "        idx_tr += idx[:n_tr]; idx_te += idx[n_tr:]\n",
    "    random.shuffle(idx_tr); random.shuffle(idx_te)\n",
    "    return idx_tr, idx_te\n",
    "\n",
    "idx_tr, idx_te = stratified_train_test_indices(base, 0.80, seed=2025)\n",
    "\n",
    "# 10% del train para validación (no toca test)\n",
    "VAL_FRAC = 0.10\n",
    "n_val = max(1, int(len(idx_tr)*VAL_FRAC))\n",
    "idx_val = idx_tr[:n_val]\n",
    "idx_tr2 = idx_tr[n_val:]\n",
    "\n",
    "len(idx_tr2), len(idx_val), len(idx_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ad275",
   "metadata": {},
   "source": [
    "Este bloque prepara la versión final del dataset ya normalizado y dividido en subconjuntos listos para entrenamiento, validación y prueba.\n",
    "\n",
    "1. **Selección del dataset procesado (`RAW_DIR`):**  \n",
    "   Se actualiza la ruta para usar el dataset reconstruido y redimensionado previamente (imágenes ya normalizadas a 256×256).  \n",
    "   Esto asegura consistencia visual y evita problemas de tamaño.\n",
    "\n",
    "2. **Lectura de clases:**  \n",
    "   Se carga el dataset con `ImageFolder` para:\n",
    "   - Obtener la lista de clases (`classes`).\n",
    "   - Calcular el número total de clases (`num_classes`).  \n",
    "   Esta información es necesaria para definir la capa de salida del modelo.\n",
    "\n",
    "3. **Definición de transformaciones (`train_tf` y `eval_tf`):**  \n",
    "   Se configuran transformaciones ligeras y rápidas:\n",
    "   - `train_tf`: incluye aumentos de datos (recorte aleatorio con padding, flips horizontales) para mejorar la capacidad de generalización del modelo durante el entrenamiento.\n",
    "   - `eval_tf`: usa transformaciones determinísticas (resize + center crop) para evaluación consistente en validación y prueba.\n",
    "   - En ambos casos se normaliza con las medias y desviaciones estándar típicas de ImageNet.\n",
    "\n",
    "4. **Partición estratificada 80/20 (`stratified_train_test_indices`):**  \n",
    "   Se define una función que divide el dataset en entrenamiento y prueba manteniendo el balance de clases:\n",
    "   - Se usa una semilla fija para reproducibilidad.\n",
    "   - Se asegura que cada clase contribuya proporcionalmente a ambos subconjuntos.\n",
    "   - Devuelve listas de índices: `idx_tr` (train) e `idx_te` (test).\n",
    "\n",
    "5. **Separación de validación:**  \n",
    "   Del conjunto de entrenamiento (`idx_tr`), se toma un **10% adicional** para construir un subconjunto de **validación (`idx_val`)**, sin tocar el conjunto de prueba.  \n",
    "   - `idx_tr2` → entrenamiento final.  \n",
    "   - `idx_val` → validación.  \n",
    "   - `idx_te` → prueba.\n",
    "\n",
    "6. **Verificación de tamaños:**  \n",
    "   Al final se imprimen las longitudes de cada split para confirmar que las proporciones quedaron correctas:\n",
    "   - número de imágenes de entrenamiento (`idx_tr2`),  \n",
    "   - número de imágenes de validación (`idx_val`),  \n",
    "   - número de imágenes de prueba (`idx_te`).\n",
    "\n",
    "Con esto quedan definidos los tres conjuntos principales que se usarán durante todo el experimento: **train / val / test** de forma consistente y reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80162da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Subset(datasets.ImageFolder(RAW_DIR, transform=train_tf), idx_tr2)\n",
    "val_ds   = Subset(datasets.ImageFolder(RAW_DIR, transform=eval_tf),  idx_val)\n",
    "test_ds  = Subset(datasets.ImageFolder(RAW_DIR, transform=eval_tf),  idx_te)\n",
    "\n",
    "BATCH = 64\n",
    "NUM_WORKERS = 0      # luego puedes intentar 2 si ya está estable\n",
    "PIN_MEMORY = False   # idem, puedes probar True más tarde\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# Preflight rápido\n",
    "xb, yb = next(iter(train_dl))\n",
    "print(\"Batch:\", xb.shape, \"| labels:\", yb.shape, \"| clases:\", num_classes)\n",
    "assert xb.shape[1]==3, f\"Se esperaban 3 canales, llegó {xb.shape}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94c22d",
   "metadata": {},
   "source": [
    "En este bloque se terminan de preparar los `DataLoader` para cada partición del dataset y se valida que los datos tengan el formato correcto antes de entrenar.\n",
    "\n",
    "1. **Creación de subconjuntos etiquetados:**\n",
    "   - `train_ds`: usa los índices de entrenamiento (`idx_tr2`) y las transformaciones de entrenamiento (`train_tf`).\n",
    "   - `val_ds`: usa los índices de validación (`idx_val`) y las transformaciones de evaluación (`eval_tf`).\n",
    "   - `test_ds`: usa los índices de prueba (`idx_te`) con las mismas transformaciones determinísticas de evaluación.\n",
    "\n",
    "   Esto garantiza que cada split vea exactamente las imágenes que le corresponden y con el preprocesamiento adecuado.\n",
    "\n",
    "2. **Parámetros de carga en lote (mini-batches):**\n",
    "   - `BATCH = 64`: número de imágenes por batch.\n",
    "   - `NUM_WORKERS = 0`: carga de datos en el proceso principal (puede subirse más adelante para acelerar).\n",
    "   - `PIN_MEMORY = False`: configuración inicial pensada para estabilidad; se puede activar más tarde para optimizar en GPU.\n",
    "\n",
    "3. **Creación de los `DataLoader`:**\n",
    "   - `train_dl`: baraja los datos (`shuffle=True`) para que el modelo no vea siempre el mismo orden.\n",
    "   - `val_dl` y `test_dl`: no barajan (`shuffle=False`) porque se usan para evaluación.\n",
    "\n",
    "   Estos `DataLoader` son los que se usarán en el loop de entrenamiento y en la evaluación por época.\n",
    "\n",
    "4. **Preflight / sanity check:**\n",
    "   - Se toma un batch de `train_dl` y se imprime su forma (`shape`) para verificar:\n",
    "     - Dimensiones esperadas: `[batch_size, canales, alto, ancho]`.\n",
    "     - Que las etiquetas (`yb`) tienen el tamaño correcto.\n",
    "     - Que las imágenes están en color RGB (3 canales).  \n",
    "   - La línea `assert xb.shape[1]==3` actúa como una verificación de seguridad: confirma que las imágenes tienen 3 canales (no escala de grises, no imágenes corruptas).\n",
    "\n",
    "En resumen, este paso confirma que el pipeline de datos está funcional y consistente antes de definir y entrenar la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb1423",
   "metadata": {},
   "source": [
    "parametros del modelo y creacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def init_weights_(m, scheme=\"kaiming\"):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        if scheme == \"kaiming\": nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "        elif scheme == \"xavier\": nn.init.xavier_normal_(m.weight)\n",
    "        elif scheme == \"orthogonal\": nn.init.orthogonal_(m.weight)\n",
    "        if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "class SmallCNNv2(nn.Module):\n",
    "    def __init__(self, num_classes, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64,64,3, padding=1),  nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # 224->112\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64,128,3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # 112->56\n",
    "        )\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.gap  = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc   = nn.Linear(128, num_classes)\n",
    "        self.apply(init_weights_)\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x); x = self.block2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.gap(x).squeeze(-1).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "net = SmallCNNv2(num_classes).to(device)\n",
    "sum(p.numel() for p in net.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b575561",
   "metadata": {},
   "source": [
    "En este bloque se define y construye una **red neuronal convolucional personalizada (SmallCNNv2)** con inicialización de pesos configurable y una arquitectura ligera enfocada en tareas de clasificación de imágenes.\n",
    "\n",
    "1. **Función `init_weights_()`:**  \n",
    "   - Se encarga de aplicar **diferentes esquemas de inicialización** de pesos según el tipo de capa y el parámetro `scheme`.  \n",
    "   - Métodos disponibles:\n",
    "     - **Kaiming (He)**: ideal para activaciones *ReLU*.  \n",
    "     - **Xavier (Glorot)**: útil en redes más simétricas.  \n",
    "     - **Orthogonal**: mantiene la independencia entre filtros.  \n",
    "   - Los sesgos (`bias`) se inicializan en cero y, en el caso de capas de normalización, los pesos se fijan en uno y los sesgos en cero.\n",
    "\n",
    "2. **Clase `SmallCNNv2`:**  \n",
    "   - Define una **arquitectura compacta y eficiente** con dos bloques convolucionales principales:  \n",
    "     - **`block1`**: dos convoluciones (3×3) con 64 filtros, seguidas de *BatchNorm* y *ReLU*, más *MaxPooling* para reducir la resolución a la mitad (224 → 112).  \n",
    "     - **`block2`**: estructura similar con 128 filtros, reduciendo de nuevo la resolución (112 → 56).  \n",
    "   - Luego incorpora:\n",
    "     - Una capa de **Dropout** para regularización.  \n",
    "     - Una **piscina promedio adaptable (`AdaptiveAvgPool2d`)** que reduce el mapa de características a tamaño 1×1 sin importar la entrada.  \n",
    "     - Una **capa totalmente conectada (`Linear`)** que genera las predicciones finales de tamaño igual al número de clases.  \n",
    "   - La red aplica automáticamente la función de inicialización definida en `init_weights_()` a todas sus capas.\n",
    "\n",
    "3. **Instanciación y envío al dispositivo:**\n",
    "   - Se crea una instancia de `SmallCNNv2` con el número de clases del dataset y se envía al dispositivo de cómputo (`GPU` o `CPU`) mediante `.to(device)`.\n",
    "\n",
    "4. **Verificación de parámetros:**\n",
    "   - Se imprime el **número total de parámetros entrenables** de la red, útil para evaluar su complejidad y tamaño.\n",
    "\n",
    "En conjunto, este bloque construye una **CNN personalizada, eficiente y bien inicializada**, lista para ser entrenada dentro del experimento de comparación de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fcfc13",
   "metadata": {},
   "source": [
    "## modelo y resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c475f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "EPOCHS_MAX    = 60        # requisito (se corta antes por early stopping)\n",
    "PATIENCE      = 8\n",
    "BASE_LR       = 0.1\n",
    "MOMENTUM      = 0.9\n",
    "WEIGHT_DECAY  = 5e-4\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "opt   = torch.optim.SGD(net.parameters(), lr=BASE_LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "steps = max(1, len(train_dl))\n",
    "warm  = LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=WARMUP_EPOCHS*steps)\n",
    "cos   = CosineAnnealingLR(opt, T_max=max(1,(EPOCHS_MAX-WARMUP_EPOCHS)*steps), eta_min=BASE_LR/100)\n",
    "sched = SequentialLR(opt, [warm, cos], milestones=[WARMUP_EPOCHS*steps])\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "scaler   = torch.amp.GradScaler(\"cuda\") if use_cuda else None\n",
    "\n",
    "def train_one_epoch(model, dl):\n",
    "    model.train()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        ctx = torch.amp.autocast(\"cuda\") if use_cuda else torch.cuda.amp.autocast(enabled=False)\n",
    "        with ctx:\n",
    "            logits = model(xb)\n",
    "            loss   = loss_fn(logits, yb)\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); opt.step()\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "        sched.step()\n",
    "    return running/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss   = loss_fn(logits, yb)\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return running/total, correct/total\n",
    "\n",
    "from pathlib import Path\n",
    "Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "best_val, best_state, wait = -1.0, None, 0\n",
    "history = []\n",
    "\n",
    "SMOKE_TEST = False\n",
    "max_epochs = 1 if SMOKE_TEST else EPOCHS_MAX\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(net, train_dl)\n",
    "    val_loss, val_acc = evaluate(net, val_dl)\n",
    "    dt = time.time()-t0\n",
    "\n",
    "    history.append(dict(epoch=epoch, train_loss=tr_loss, train_acc=tr_acc,\n",
    "                        val_loss=val_loss,   val_acc=val_acc,\n",
    "                        lr=opt.param_groups[0]['lr'], time=dt))\n",
    "    print(f\"ep{epoch:02d}  tr_acc={tr_acc:.3f}  val_acc={val_acc:.3f}  lr={opt.param_groups[0]['lr']:.5f}  {dt:.1f}s\")\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val, wait = val_acc, 0\n",
    "        best_state = copy.deepcopy(net.state_dict())\n",
    "        torch.save(best_state, \"checkpoints/best.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"Early stopping en ep {epoch} (mejor val_acc={best_val:.3f})\")\n",
    "            break\n",
    "\n",
    "if best_state: \n",
    "    net.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb314da",
   "metadata": {},
   "source": [
    "Este bloque define todo el ciclo de entrenamiento, validación y control del aprendizaje del modelo, incluyendo la política de tasa de aprendizaje, el optimizador, el cálculo de la pérdida y el mecanismo de early stopping.\n",
    "\n",
    "### 1. Hiperparámetros principales\n",
    "Se establecen valores clave para el entrenamiento:\n",
    "- `EPOCHS_MAX`: número máximo de épocas planeadas.\n",
    "- `PATIENCE`: criterio de early stopping (cuántas épocas seguidas se permite no mejorar).\n",
    "- `BASE_LR`, `MOMENTUM`, `WEIGHT_DECAY`: hiperparámetros del optimizador SGD.\n",
    "- `WARMUP_EPOCHS`: número de épocas dedicadas al calentamiento del learning rate.\n",
    "\n",
    "Estos parámetros definen el comportamiento global del entrenamiento (velocidad de aprendizaje, regularización y estabilidad).\n",
    "\n",
    "### 2. Optimizador y scheduler de learning rate\n",
    "- Se crea el optimizador `SGD` con momentum y weight decay.\n",
    "- Se calcula `steps` como el número de iteraciones por época.\n",
    "- Se definen dos fases de la tasa de aprendizaje:\n",
    "  - **Warmup (`LinearLR`)**: empieza con un LR muy pequeño y lo incrementa gradualmente durante las primeras épocas para evitar inestabilidad inicial.\n",
    "  - **CosineAnnealingLR**: después del warmup, el LR va decayendo de forma suave tipo coseno hasta un valor mínimo.\n",
    "- `SequentialLR` une ambas fases en un solo scheduler (`sched`).\n",
    "\n",
    "Esto implementa una política de LR con calentamiento inicial + decaimiento progresivo, que típicamente mejora la convergencia.\n",
    "\n",
    "### 3. Función de pérdida y precisión numérica\n",
    "- `loss_fn = nn.CrossEntropyLoss()`: se usa entropía cruzada para clasificación multiclase.\n",
    "- Se detecta si hay GPU disponible (`use_cuda`) y se configura `GradScaler` para **entrenamiento con precisión mixta (mixed precision)**, lo cual acelera el entrenamiento y reduce uso de memoria en GPU.\n",
    "\n",
    "### 4. `train_one_epoch(model, dl)`\n",
    "Esta función ejecuta **una época completa de entrenamiento**:\n",
    "- Activa modo entrenamiento (`model.train()`).\n",
    "- Para cada batch:\n",
    "  - Mueve tensores a `device`.\n",
    "  - Limpia gradientes con `opt.zero_grad()`.\n",
    "  - Hace forward pass del modelo y calcula la pérdida.\n",
    "  - Hace backward y update de pesos:\n",
    "    - Si hay `scaler`, usa `scaler.scale(...).backward()` y `scaler.step(...)` para entrenamiento en precisión mixta.\n",
    "    - Si no hay GPU, usa el flujo normal (`loss.backward()` + `opt.step()`).\n",
    "  - Actualiza métricas acumuladas de pérdida promedio y accuracy por batch.\n",
    "  - Llama a `sched.step()` para avanzar la tasa de aprendizaje en cada iteración.\n",
    "- Devuelve la pérdida media de entrenamiento y la accuracy de entrenamiento.\n",
    "\n",
    "Esta función representa la fase donde el modelo **aprende y actualiza parámetros**.\n",
    "\n",
    "### 5. `evaluate(model, dl)`\n",
    "- Desactiva gradientes con `@torch.no_grad()` y pone el modelo en modo evaluación (`model.eval()`).\n",
    "- Recorre el dataloader sin hacer backprop.\n",
    "- Calcula la pérdida promedio y la accuracy en validación/prueba.\n",
    "- Esto mide **qué tan bien generaliza el modelo** sin actualizarlo.\n",
    "\n",
    "### 6. Loop de entrenamiento principal\n",
    "- Se crea la carpeta `checkpoints/` si no existe para guardar el mejor modelo.\n",
    "- Se inicializan controladores de early stopping:\n",
    "  - `best_val`: mejor accuracy de validación vista hasta ahora.\n",
    "  - `best_state`: copia de los pesos del mejor modelo.\n",
    "  - `wait`: cuántas épocas van sin mejorar.\n",
    "- `SMOKE_TEST` permite correr solo 1 época rápida para depuración.\n",
    "\n",
    "Para cada época:\n",
    "1. Se entrena una época completa (`train_one_epoch`) y luego se evalúa en validación (`evaluate`).\n",
    "2. Se guarda en `history` la información de la época: métricas de train/val, LR actual y tiempo usado.\n",
    "3. Se imprime un resumen legible con las métricas clave.\n",
    "\n",
    "### 7. Early Stopping y checkpoints\n",
    "- Si la accuracy de validación (`val_acc`) mejora:\n",
    "  - Se actualiza `best_val`.\n",
    "  - Se guarda el estado actual del modelo como `best_state`.\n",
    "  - También se guarda a disco en `\"checkpoints/best.pt\"`.\n",
    "- Si no mejora:\n",
    "  - Se incrementa `wait`.\n",
    "  - Si `wait` alcanza `PATIENCE`, se detiene el entrenamiento temprano (early stopping), asumiendo que ya no hay mejora significativa.\n",
    "\n",
    "Al final, si existe un `best_state`, se recarga en `net`.  \n",
    "Esto garantiza que el modelo final en memoria sea la **mejor versión validada**, no simplemente el último paso entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c70ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_test(model, dl):\n",
    "    model.eval()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = nn.CrossEntropyLoss()(logits, yb)\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return running/total, correct/total\n",
    "\n",
    "test_loss, test_acc = eval_test(net, test_dl)\n",
    "print(f\"[TEST] loss={test_loss:.4f}  acc={test_acc:.4f}  (best val_acc={best_val:.4f})\")\n",
    "\n",
    "if history:\n",
    "    with open(\"training_history.csv\",\"w\",newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys()); w.writeheader(); w.writerows(history)\n",
    "    with open(\"best_summary.json\",\"w\") as f:\n",
    "        json.dump(dict(best_val_acc=best_val, test_acc=test_acc,\n",
    "                       epochs_run=len(history),\n",
    "                       hp=dict(lr=BASE_LR, momentum=0.9, wd=5e-4,\n",
    "                               scheduler=\"cosine+warmup\", batch=BATCH)),\n",
    "                  f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111e6a3",
   "metadata": {},
   "source": [
    "Este bloque realiza la evaluación final del modelo en el conjunto de prueba y guarda los resultados principales para documentación y reproducibilidad.\n",
    "\n",
    "### 1. Función `eval_test(model, dl)`\n",
    "- Se define una función para evaluar el modelo sobre el **conjunto de prueba (test)**.\n",
    "- Se marca con `@torch.no_grad()` y `model.eval()` para desactivar el cálculo de gradientes y asegurarse de que el modelo está en modo inferencia.\n",
    "- Para cada batch del dataloader:\n",
    "  - Se mueven las imágenes y etiquetas al dispositivo (`device`).\n",
    "  - Se ejecuta el modelo para obtener `logits`.\n",
    "  - Se calcula la pérdida de clasificación (`CrossEntropyLoss`).\n",
    "  - Se acumulan:\n",
    "    - La pérdida total ponderada.\n",
    "    - El número de predicciones correctas.\n",
    "    - El número total de ejemplos.\n",
    "- Al final, regresa la **pérdida promedio en test** y la **accuracy en test**.\n",
    "\n",
    "Esta función mide el rendimiento final del modelo en datos nunca vistos durante entrenamiento o validación.\n",
    "\n",
    "### 2. Métrica final de prueba\n",
    "- Se llama `eval_test(net, test_dl)` usando la mejor versión del modelo (`net` ya recargado con `best_state`).\n",
    "- Se imprime:\n",
    "  - `test_loss`: pérdida promedio en el conjunto de prueba.\n",
    "  - `test_acc`: accuracy final en test.\n",
    "  - `best_val`: mejor accuracy de validación obtenida durante el entrenamiento.\n",
    "  \n",
    "Esto permite comparar desempeño en validación vs. desempeño real en prueba.\n",
    "\n",
    "### 3. Exportación de resultados\n",
    "Si existe historial de entrenamiento (`history`):\n",
    "- Se guarda `training_history.csv` con las métricas por época (loss/acc de train y val, learning rate, tiempo).  \n",
    "  Esto sirve para graficar curvas después sin volver a entrenar.\n",
    "- Se guarda `best_summary.json` con:\n",
    "  - `best_val_acc`: mejor accuracy en validación.\n",
    "  - `test_acc`: accuracy final en prueba.\n",
    "  - `epochs_run`: cuántas épocas se entrenaron antes de hacer early stopping.\n",
    "  - `hp`: resumen de hiperparámetros importantes (learning rate base, momentum, weight decay, tipo de scheduler y tamaño de batch).\n",
    "\n",
    "En conjunto, este bloque produce un **cierre reproducible del experimento**: evalúa el modelo final en test y deja guardados los resultados y la configuración usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "assert history, \"No hay history\"\n",
    "\n",
    "epochs = [h[\"epoch\"] for h in history]\n",
    "tr_loss = [h[\"train_loss\"] for h in history]\n",
    "va_loss = [h[\"val_loss\"]   for h in history]\n",
    "tr_acc  = [h[\"train_acc\"]  for h in history]\n",
    "va_acc  = [h[\"val_acc\"]    for h in history]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(epochs, tr_loss, label=\"train\"); plt.plot(epochs, va_loss, label=\"val\")\n",
    "plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2); plt.plot(epochs, tr_acc, label=\"train\"); plt.plot(epochs, va_acc, label=\"val\")\n",
    "plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc\"); plt.legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7f376",
   "metadata": {},
   "source": [
    "Este bloque genera las gráficas de entrenamiento para analizar el comportamiento del modelo a lo largo de las épocas.\n",
    "\n",
    "1. **Extracción de métricas:**\n",
    "   - Se toma la información guardada en `history` (una lista de diccionarios recopilada durante el entrenamiento).\n",
    "   - Se extraen, para cada época:\n",
    "     - `train_loss`: pérdida en entrenamiento.\n",
    "     - `val_loss`: pérdida en validación.\n",
    "     - `train_acc`: accuracy en entrenamiento.\n",
    "     - `val_acc`: accuracy en validación.\n",
    "   - También se extrae el número de época (`epoch`) para usarlo como eje X.\n",
    "\n",
    "2. **Gráfica de pérdida (`Loss`):**\n",
    "   - Muestra cómo cambia la pérdida de entrenamiento y validación con las épocas.\n",
    "   - Sirve para evaluar si el modelo sigue aprendiendo, si se estanca o si hay sobreajuste (por ejemplo, pérdida de entrenamiento bajando pero pérdida de validación quedándose alta o subiendo).\n",
    "\n",
    "3. **Gráfica de exactitud (`Accuracy`):**\n",
    "   - Muestra cómo evoluciona la precisión tanto en entrenamiento como en validación.\n",
    "   - Esto permite observar la ganancia de desempeño y si existe brecha entre train y val (generalization gap).\n",
    "\n",
    "4. **Interpretación visual (según las gráficas mostradas):**\n",
    "   - La pérdida de entrenamiento desciende de forma consistente, indicando que el modelo está optimizando correctamente.\n",
    "   - La pérdida de validación baja inicialmente y luego oscila, lo que sugiere que después de cierto punto el modelo puede estar comenzando a ajustar demasiado al conjunto de entrenamiento.\n",
    "   - La accuracy de entrenamiento sigue creciendo de manera sostenida.\n",
    "   - La accuracy de validación también mejora al inicio y luego se estabiliza en un rango más bajo que la de entrenamiento, lo que indica una posible aparición de **sobreajuste moderado**.\n",
    "\n",
    "Estas gráficas se usan después en el análisis para justificar decisiones como regularización extra, cambio de scheduler o ajuste del número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2483620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_all(model, dl, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            yhat = logits.argmax(1).cpu().numpy()\n",
    "            y_true.append(yb.numpy()); y_pred.append(yhat)\n",
    "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
    "    return y_true, y_pred\n",
    "\n",
    "y_true, y_pred = predict_all(net, test_dl, device)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred, target_names=classes, digits=3))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion matrix (test)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicho\"); plt.ylabel(\"Real\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97327c04",
   "metadata": {},
   "source": [
    "Este bloque evalúa el desempeño del modelo a nivel de clase y visualiza en qué clases se confunde más.\n",
    "\n",
    "### 1. Función `predict_all(model, dl, device)`\n",
    "- Recorre todo el dataloader de prueba (`test_dl`) en modo evaluación.\n",
    "- Para cada batch:\n",
    "  - Envía las imágenes al dispositivo (`device`).\n",
    "  - Obtiene las salidas del modelo (`logits`).\n",
    "  - Convierte cada salida en una predicción de clase tomando el índice con la probabilidad más alta (`argmax`).\n",
    "- Regresa dos arreglos:\n",
    "  - `y_true`: las etiquetas reales.\n",
    "  - `y_pred`: las etiquetas predichas por el modelo.\n",
    "\n",
    "Esto da todas las predicciones del modelo sobre el conjunto de prueba.\n",
    "\n",
    "### 2. Métricas por clase\n",
    "- Se usa `classification_report` para imprimir, por cada clase:\n",
    "  - **precision:** de las imágenes clasificadas como esa clase, qué fracción eran correctas.\n",
    "  - **recall:** de las imágenes que realmente son de esa clase, cuántas se detectaron bien.\n",
    "  - **f1-score:** balance entre precision y recall.\n",
    "  - **support:** cuántos ejemplos había de esa clase en el test.\n",
    "- También muestra métricas globales como `accuracy` y promedios macro/weighted.\n",
    "\n",
    "Esto permite ver en qué clases el modelo funciona mejor y en cuáles falla más.\n",
    "\n",
    "### 3. Matriz de confusión\n",
    "- Se construye la matriz de confusión `cm = confusion_matrix(y_true, y_pred)`.\n",
    "- Se grafica como una imagen coloreada (heatmap):\n",
    "  - Eje Y: clase real.\n",
    "  - Eje X: clase predicha.\n",
    "  - Cada celda indica cuántas veces una clase real fue clasificada como otra clase.\n",
    "- Se etiquetan los ejes con los nombres de las clases para interpretar el patrón de errores.\n",
    "\n",
    "### 4. Interpretación visual (según la figura mostrada)\n",
    "- Las celdas diagonales brillantes indican clases que el modelo acierta con frecuencia.\n",
    "- Valores fuera de la diagonal muestran confusiones entre clases específicas.\n",
    "- Esto ayuda a detectar pares de clases que el modelo tiende a mezclar (por ejemplo, animales similares o personajes visualmente parecidos).\n",
    "\n",
    "En resumen, este bloque ofrece una **evaluación detallada a nivel de clase** y una **diagnosis de errores**, útil para justificar mejoras futuras (más datos, más regularización, ajuste de arquitectura, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca8500",
   "metadata": {},
   "source": [
    "## testeo de hyperparametros y optimizadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95646fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Banco de Optimizadores con 3 seeds (SGD, SGD+Nesterov, Adam, AdamW) =====\n",
    "import os, time, math, copy, json, csv, random, numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- util: reproducibilidad\n",
    "def set_seed(seed=1337, deterministic=False):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = deterministic\n",
    "    torch.backends.cudnn.benchmark = not deterministic\n",
    "\n",
    "# --- modelo fresco por corrida\n",
    "def fresh_model(num_classes):\n",
    "    m = SmallCNNv2(num_classes).to(device)  # tu CNN v2 (2 bloques conv + GAP)\n",
    "    return m\n",
    "\n",
    "# --- fábricas de optimizador y scheduler (cosine + warmup)\n",
    "def make_opt(name, params):\n",
    "    name = name.lower()\n",
    "    if name == \"sgd\":\n",
    "        return torch.optim.SGD(params, lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=False)\n",
    "    if name == \"sgd_nesterov\":\n",
    "        return torch.optim.SGD(params, lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "    if name == \"adam\":\n",
    "        return torch.optim.Adam(params, lr=1e-3, weight_decay=0.0)\n",
    "    if name == \"adamw\":\n",
    "        return torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-2)\n",
    "    raise ValueError(name)\n",
    "\n",
    "def make_sched(opt, epochs, steps_per_epoch, warmup_epochs=5, eta_min=None, base_lr=None, is_adam_family=False):\n",
    "    if eta_min is None:\n",
    "        eta_min = (base_lr/100) if base_lr else (1e-5 if is_adam_family else 1e-3)\n",
    "    warm = LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=max(1, warmup_epochs*steps_per_epoch))\n",
    "    cos  = CosineAnnealingLR(opt, T_max=max(1,(epochs-warmup_epochs)*steps_per_epoch), eta_min=eta_min)\n",
    "    return SequentialLR(opt, [warm, cos], milestones=[warmup_epochs*steps_per_epoch])\n",
    "\n",
    "# --- loop de entrenamiento/validación (AMP nueva)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "loss_fn  = nn.CrossEntropyLoss()\n",
    "scaler   = torch.amp.GradScaler(\"cuda\") if use_cuda else None\n",
    "\n",
    "def train_one_epoch(model, dl, opt, sched):\n",
    "    model.train()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        ctx = torch.amp.autocast(\"cuda\") if use_cuda else torch.cuda.amp.autocast(enabled=False)\n",
    "        with ctx:\n",
    "            logits = model(xb)\n",
    "            loss   = loss_fn(logits, yb)\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); opt.step()\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "        sched.step()\n",
    "    return running/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss   = loss_fn(logits, yb)\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return running/total, correct/total\n",
    "\n",
    "# --- logger estilo \"[INFO] ...\" con timestamp\n",
    "def log(msg):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S,%f\")[:-3]\n",
    "    print(f\"{ts} [INFO] {msg}\")\n",
    "\n",
    "# --- runner para UNA condición (optimizer) y UNA seed\n",
    "def run_once(opt_name, seed, epochs=20, patience=8):\n",
    "    set_seed(seed, deterministic=False)\n",
    "    model = fresh_model(num_classes)\n",
    "    opt   = make_opt(opt_name, model.parameters())\n",
    "\n",
    "    base_lr   = opt.param_groups[0]['lr']\n",
    "    is_adam_f = any(isinstance(opt, c) for c in (torch.optim.Adam, torch.optim.AdamW))\n",
    "    steps     = max(1, len(train_dl))\n",
    "    sched     = make_sched(opt, epochs=epochs, steps_per_epoch=steps,\n",
    "                           warmup_epochs=5, eta_min=None, base_lr=base_lr, is_adam_family=is_adam_f)\n",
    "\n",
    "    best_val, best_state, wait = -1.0, None, 0\n",
    "    history = []\n",
    "    t0 = time.time()\n",
    "    for ep in range(epochs):\n",
    "        tr_loss, tr_acc   = train_one_epoch(model, train_dl, opt, sched)\n",
    "        val_loss, val_acc = evaluate(model, val_dl)\n",
    "        lr_now = opt.param_groups[0]['lr']\n",
    "        history.append(dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc,\n",
    "                            val_loss=val_loss, val_acc=val_acc, lr=lr_now))\n",
    "        log(f\"Epoch {ep+1}/{epochs} | Train Loss: {tr_loss:.3f}, Train Acc: {100*tr_acc:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.3f}, Val Acc: {100*val_acc:.2f}% | LR: {lr_now:.2e}\")\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val, wait = val_acc, 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                log(f\"--- Early stop en ep {ep+1} (mejor val_acc={100*best_val:.2f}%) ---\")\n",
    "                break\n",
    "\n",
    "    total_min = (time.time()-t0)/60.0\n",
    "    log(f\"--- Finished: {opt_name}_seed{seed} ---\")\n",
    "    log(f\"Best Val Acc: {100*best_val:.2f}%\")\n",
    "    log(f\"Total Time: {total_min:.2f} minutes\")\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_loss, test_acc = evaluate(model, test_dl)\n",
    "\n",
    "    # guarda history por si quieres graficar luego\n",
    "    os.makedirs(\"runs\", exist_ok=True)\n",
    "    with open(f\"runs/{opt_name}_seed{seed}_history.csv\",\"w\",newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys()); w.writeheader(); w.writerows(history)\n",
    "\n",
    "    return dict(opt=opt_name, seed=seed, best_val=best_val, test_acc=test_acc, minutes=total_min)\n",
    "\n",
    "# --- experimento: 3 seeds x 4 optimizadores\n",
    "SEEDS = [1337, 2025, 777]\n",
    "OPTIMIZERS = [\"sgd\", \"sgd_nesterov\", \"adam\", \"adamw\"]\n",
    "EPOCHS_PER_RUN = 20   # rápido para explorar; para el “oficial” usa 60 como pide la práctica\n",
    "\n",
    "all_runs = []\n",
    "for name in OPTIMIZERS:\n",
    "    results_seed = []\n",
    "    for sd in SEEDS:\n",
    "        log(f\"=== {name} (seed {sd}) ===\")\n",
    "        res = run_once(name, sd, epochs=EPOCHS_PER_RUN, patience=8)\n",
    "        results_seed.append(res)\n",
    "    # agrega resumen por optimizador\n",
    "    v = np.array([r[\"best_val\"]  for r in results_seed])\n",
    "    t = np.array([r[\"test_acc\"]  for r in results_seed])\n",
    "    log(f\"--- RESULT for {name}: Val {100*v.mean():.2f} ± {100*v.std():.2f} | Test {100*t.mean():.2f} ± {100*t.std():.2f}\")\n",
    "    all_runs += results_seed\n",
    "\n",
    "# --- tabla final (mean ± std por optimizador)\n",
    "from collections import defaultdict\n",
    "group = defaultdict(list)\n",
    "for r in all_runs:\n",
    "    group[r[\"opt\"]].append(r)\n",
    "\n",
    "print(\"\\n\" + \"=\"*49)\n",
    "print(\"--- FINAL RESULTS (Best Val Acc, Mean ± Std over 3 seeds) ---\")\n",
    "print(\"=\"*49)\n",
    "rows = []\n",
    "for name in OPTIMIZERS:\n",
    "    vals = np.array([x[\"best_val\"] for x in group[name]])\n",
    "    tes  = np.array([x[\"test_acc\"] for x in group[name]])\n",
    "    line = f\"{name:<24} | Val {100*vals.mean():.2f} ± {100*vals.std():.2f} | Test {100*tes.mean():.2f} ± {100*tes.std():.2f}\"\n",
    "    print(line)\n",
    "    rows.append(dict(opt=name, val_mean=float(vals.mean()), val_std=float(vals.std()),\n",
    "                     test_mean=float(tes.mean()), test_std=float(tes.std())))\n",
    "\n",
    "with open(\"runs/optimizers_summary.csv\",\"w\",newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=rows[0].keys()); w.writeheader(); w.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0a4ee",
   "metadata": {},
   "source": [
    "Este bloque implementa un experimento de comparación de optimizadores. Ejecuta múltiples entrenamientos controlados variando únicamente el tipo de optimizador (SGD, SGD+Nesterov, Adam, AdamW) y repitiendo cada caso con distintas semillas aleatorias. El objetivo es medir estabilidad, rapidez de convergencia y desempeño final.\n",
    "\n",
    "### 1. Configuración base y utilidades\n",
    "- Se importan las librerías necesarias (`torch`, `numpy`, `time`, `csv`, etc.) y se detecta el dispositivo (`device`).\n",
    "- `set_seed(seed, deterministic=False)` establece las semillas de aleatoriedad para asegurar reproducibilidad entre corridas, y permite opcionalmente forzar comportamiento determinista de CuDNN.\n",
    "\n",
    "### 2. Creación de modelos \"desde cero\"\n",
    "- `fresh_model(num_classes)` crea una nueva instancia del modelo `SmallCNNv2` y la envía a `device`.\n",
    "- Esto garantiza que cada corrida empiece con pesos nuevos (no reutiliza pesos de entrenamientos anteriores), lo cual es importante para comparar optimizadores de manera justa.\n",
    "\n",
    "### 3. Fábricas de optimizador y scheduler\n",
    "- `make_opt(name, params)` devuelve una instancia del optimizador solicitado:\n",
    "  - `sgd`: SGD clásico con momentum.\n",
    "  - `sgd_nesterov`: SGD con Nesterov.\n",
    "  - `adam`: Adam estándar.\n",
    "  - `adamw`: AdamW (Adam con decoupled weight decay).\n",
    "  Cada optimizador viene con hiperparámetros típicos recomendados (LR, momentum, weight decay).\n",
    "  \n",
    "- `make_sched(...)` construye una política de **learning rate** con:\n",
    "  - Fase de *warmup* lineal (LR pequeño → LR base).\n",
    "  - Fase de *cosine annealing* (decaimiento suave del LR).\n",
    "  \n",
    "  Esto replica la misma política de tasa de aprendizaje para todas las corridas, de modo que el factor que cambia sea solo el optimizador.\n",
    "\n",
    "### 4. Ciclo de entrenamiento y validación\n",
    "- Se reutilizan las funciones `train_one_epoch` y `evaluate`:\n",
    "  - `train_one_epoch(...)` realiza una época completa de entrenamiento con ese optimizador y su scheduler.\n",
    "    - Usa precisión mixta (AMP) cuando hay GPU disponible para acelerar y estabilizar el entrenamiento.\n",
    "    - Devuelve pérdida promedio y accuracy en entrenamiento.\n",
    "  - `evaluate(...)` evalúa el modelo en validación sin actualizar pesos.\n",
    "    - Devuelve pérdida promedio y accuracy en validación.\n",
    "\n",
    "- Ambas funciones calculan métricas agregadas por época (loss y accuracy) que luego se registran para análisis.\n",
    "\n",
    "### 5. Ejecución controlada de una corrida (`run_once`)\n",
    "`run_once(opt_name, seed, epochs, patience)` hace lo siguiente para una sola combinación de optimizador y semilla:\n",
    "1. Fija la semilla (`set_seed`).\n",
    "2. Crea un modelo nuevo (`fresh_model`).\n",
    "3. Construye el optimizador adecuado (`make_opt`) y su scheduler asociado (`make_sched`).\n",
    "4. Entrena durante un número fijo de épocas (`epochs`), registrando en cada época:\n",
    "   - pérdida y accuracy de entrenamiento,\n",
    "   - pérdida y accuracy de validación,\n",
    "   - learning rate actual.\n",
    "5. Mantiene seguimiento del **mejor accuracy de validación** visto hasta el momento:\n",
    "   - Si mejora, guarda el estado de la red como `best_state`.\n",
    "   - Si no mejora por varias épocas consecutivas (`patience`), activa *early stopping*.\n",
    "6. Al final, recarga los mejores pesos validados y evalúa en el conjunto de prueba (`test_dl`), obteniendo la accuracy final de test.\n",
    "7. Guarda el historial de métricas por época en CSV (una fila por época) para poder graficar después.\n",
    "\n",
    "Devuelve un resumen con:\n",
    "- nombre del optimizador,\n",
    "- semilla usada,\n",
    "- mejor accuracy de validación,\n",
    "- accuracy final en test,\n",
    "- tiempo total de la corrida.\n",
    "\n",
    "### 6. Bucle del experimento completo\n",
    "- Se definen:\n",
    "  - `SEEDS`: lista de 3 semillas distintas.\n",
    "  - `OPTIMIZERS`: los cuatro optimizadores a evaluar.\n",
    "  - `EPOCHS_PER_RUN`: cuántas épocas entrenar por corrida (se puede subir a 60 para el experimento “oficial” del laboratorio).\n",
    "  \n",
    "- Para cada optimizador:\n",
    "  - Se entrenan 3 modelos distintos (uno por cada semilla).\n",
    "  - Para cada corrida se imprime en consola un log de las métricas por época y el estado de early stopping.\n",
    "  - Después de las 3 seeds, se calcula la media y desviación estándar del accuracy de validación y del accuracy de test para ese optimizador.  \n",
    "    Esto mide **rendimiento promedio** y **estabilidad entre seeds**.\n",
    "\n",
    "- Todos los resultados individuales se van guardando en `all_runs`.\n",
    "\n",
    "### 7. Resumen final y exportación\n",
    "- Se agrupan los resultados por optimizador y se construye una tabla final:\n",
    "  - `val_mean ± val_std`: accuracy promedio en validación y su variabilidad.\n",
    "  - `test_mean ± test_std`: accuracy promedio final en test y su variabilidad.\n",
    "- Esa tabla se imprime para comparar directamente los optimizadores.\n",
    "- También se guarda un archivo `optimizers_summary.csv` con el resumen por optimizador, para dejar evidencia cuantitativa del estudio.\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "Este bloque automatiza el estudio del eje “Optimizador” del laboratorio.  \n",
    "Ejecuta corridas repetibles, registra métricas, aplica early stopping controlado, y produce promedios ± desviación estándar.  \n",
    "Eso permite evaluar no solo qué optimizador rinde más alto, sino cuál es más consistente entre semillas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78af3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, copy, time, csv, json\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torchvision.models import alexnet, AlexNet_Weights, vgg16, VGG16_Weights\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "def build_finetune_model(arch: str, num_classes: int, mode: str = \"head\"):\n",
    "    \"\"\"\n",
    "    arch: 'alexnet' | 'vgg16'\n",
    "    mode:\n",
    "      - 'head'       -> congela todo menos la cabeza\n",
    "      - 'last_block' -> descongela solo el último bloque conv (y la cabeza)\n",
    "    \"\"\"\n",
    "    arch = arch.lower()\n",
    "    if arch == \"alexnet\":\n",
    "        m = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.classifier[6].in_features\n",
    "        m.classifier[6] = nn.Linear(in_feats, num_classes)\n",
    "        # freeze\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = False\n",
    "        # head trainable\n",
    "        for p in m.classifier[6].parameters():\n",
    "            p.requires_grad = True\n",
    "        if mode == \"last_block\":\n",
    "            # último bloque conv (aprox. conv5)\n",
    "            for p in m.features[8:].parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    elif arch == \"vgg16\":\n",
    "        m = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        in_feats = m.classifier[6].in_features\n",
    "        m.classifier[6] = nn.Linear(in_feats, num_classes)\n",
    "        # freeze\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = False\n",
    "        # head trainable\n",
    "        for p in m.classifier[6].parameters():\n",
    "            p.requires_grad = True\n",
    "        if mode == \"last_block\":\n",
    "            # conv5_x\n",
    "            for p in m.features[24:].parameters():\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        raise ValueError(\"arch debe ser 'alexnet' o 'vgg16'\")\n",
    "\n",
    "    return m\n",
    "\n",
    "def make_opt_and_sched(model, epochs=30, warmup_epochs=3):\n",
    "    # 2 grupos: base (feature extractor) LR más bajo; cabeza LR más alto\n",
    "    head_params = []\n",
    "    base_params = []\n",
    "    for n,p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if \"classifier.6\" in n:\n",
    "                head_params.append(p)\n",
    "            else:\n",
    "                base_params.append(p)\n",
    "    # AdamW con LR diferencial\n",
    "    opt = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": base_params, \"lr\": 1e-4, \"weight_decay\": 1e-4},\n",
    "            {\"params\": head_params, \"lr\": 3e-4, \"weight_decay\": 1e-4},\n",
    "        ]\n",
    "    )\n",
    "    steps = max(1, len(train_dl))\n",
    "    warm  = LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=warmup_epochs*steps)\n",
    "    cos   = CosineAnnealingLR(opt, T_max=max(1,(epochs-warmup_epochs)*steps), eta_min=1e-6)\n",
    "    sched = SequentialLR(opt, [warm, cos], milestones=[warmup_epochs*steps])\n",
    "    return opt, sched\n",
    "\n",
    "def run_finetune(arch, mode=\"head\", epochs=30, patience=6, tag=None):\n",
    "    model = build_finetune_model(arch, num_classes, mode).to(device)\n",
    "    opt, sched = make_opt_and_sched(model, epochs=epochs, warmup_epochs=3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    scaler   = torch.amp.GradScaler(\"cuda\") if use_cuda else None\n",
    "\n",
    "    # reusar tus train_one_epoch/evaluate, pero con estos opt/sched/loss_fn\n",
    "    def train_one_epoch_ft():\n",
    "        model.train(); total=correct=0; running=0.0\n",
    "        for xb,yb in train_dl:\n",
    "            xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            ctx = torch.amp.autocast(\"cuda\") if use_cuda else torch.cuda.amp.autocast(enabled=False)\n",
    "            with ctx:\n",
    "                logits = model(xb)\n",
    "                loss   = loss_fn(logits, yb)\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); opt.step()\n",
    "            running += loss.item()*xb.size(0)\n",
    "            correct += (logits.argmax(1)==yb).sum().item()\n",
    "            total   += xb.size(0)\n",
    "            sched.step()\n",
    "        return running/total, correct/total\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_ft(dl):\n",
    "        model.eval(); total=correct=0; running=0.0\n",
    "        for xb,yb in dl:\n",
    "            xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            logits = model(xb)\n",
    "            loss   = loss_fn(logits, yb)\n",
    "            running += loss.item()*xb.size(0)\n",
    "            correct += (logits.argmax(1)==yb).sum().item()\n",
    "            total   += xb.size(0)\n",
    "        return running/total, correct/total\n",
    "\n",
    "    best_val, best_state, wait = -1.0, None, 0\n",
    "    history = []\n",
    "    t0 = time.time()\n",
    "    for ep in range(epochs):\n",
    "        tr_loss, tr_acc   = train_one_epoch_ft()\n",
    "        val_loss, val_acc = evaluate_ft(val_dl)\n",
    "        history.append(dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc,\n",
    "                            val_loss=val_loss,   val_acc=val_acc,\n",
    "                            lr=opt.param_groups[0]['lr']))\n",
    "        print(f\"[{arch}:{mode}] ep{ep:02d} tr_acc={tr_acc:.3f} val_acc={val_acc:.3f} lr={opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val, wait = val_acc, 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "            torch.save(best_state, f\"checkpoints/{arch}_{mode}_best.pt\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"[{arch}:{mode}] Early stopping en ep {ep} (best val={best_val:.3f})\")\n",
    "                break\n",
    "\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "    test_loss, test_acc = evaluate_ft(test_dl)\n",
    "    total_min = (time.time()-t0)/60.0\n",
    "    tag = tag or f\"{arch}_{mode}\"\n",
    "    # guardar historia y resumen\n",
    "    Path(\"runs\").mkdir(exist_ok=True)\n",
    "    with open(f\"runs/{tag}_history.csv\",\"w\",newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys()); w.writeheader(); w.writerows(history)\n",
    "    with open(f\"runs/{tag}_summary.json\",\"w\") as f:\n",
    "        json.dump(dict(tag=tag, best_val=float(best_val), test_acc=float(test_acc),\n",
    "                       epochs_run=len(history), minutes=total_min), f, indent=2)\n",
    "\n",
    "    print(f\"[{tag}] BEST val={best_val:.3f} | TEST acc={test_acc:.3f} | time={total_min:.1f}m\")\n",
    "    return model, dict(best_val=best_val, test_acc=test_acc, history=history)\n",
    "\n",
    "# === Ejecuta pruebas ===\n",
    "# Opción 1: solo cabeza (rápida, muy estable)\n",
    "alex_head, alex_head_res = run_finetune(\"alexnet\", mode=\"head\",       epochs=30, patience=6, tag=\"alexnet_head\")\n",
    "\n",
    "# Opción 2: descongelar último bloque conv (algo más lenta, a veces mejora)\n",
    "alex_blk,  alex_blk_res  = run_finetune(\"alexnet\", mode=\"last_block\", epochs=30, patience=6, tag=\"alexnet_lastblk\")\n",
    "\n",
    "# VGG16 (más pesado: si va lento, baja a epochs=20)\n",
    "vgg_head,  vgg_head_res  = run_finetune(\"vgg16\",   mode=\"head\",       epochs=30, patience=6, tag=\"vgg16_head\")\n",
    "vgg_blk,   vgg_blk_res   = run_finetune(\"vgg16\",   mode=\"last_block\", epochs=30, patience=6, tag=\"vgg16_lastblk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c2400",
   "metadata": {},
   "source": [
    "Este bloque implementa fine-tuning con arquitecturas preentrenadas (AlexNet y VGG16) sobre el conjunto de datos personalizado.  \n",
    "El objetivo es comparar dos modos de ajuste: entrenar solo la cabeza clasificadora o desbloquear parte del extractor de características.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Carga y preparación de modelos preentrenados (`build_finetune_model`)\n",
    "- La función `build_finetune_model(arch, num_classes, mode)` construye una red basada en `alexnet` o `vgg16` con pesos preentrenados en ImageNet.\n",
    "- Se reemplaza la última capa fully-connected (`classifier[6]`) por una capa nueva con `num_classes` salidas, para adaptar el modelo a nuestras clases.\n",
    "- Se controla qué partes del modelo se entrenan:\n",
    "  - `mode=\"head\"`  \n",
    "    - Se **congelan todos los parámetros** (`requires_grad = False`) para mantener el extractor de características original.  \n",
    "    - Solo se entrena la nueva capa final (cabeza clasificadora).\n",
    "  - `mode=\"last_block\"`  \n",
    "    - Además de la cabeza, se **descongela el último bloque convolucional** (por ejemplo, las capas finales del feature extractor).  \n",
    "    - Esto permite que parte del extractor se adapte a nuestro dominio visual, a costa de más cómputo y riesgo de overfitting.\n",
    "\n",
    "Este control nos deja estudiar qué tan necesario es ajustar internamente el backbone vs. entrenar solo la parte final.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Optimizador y scheduler con LR diferenciado (`make_opt_and_sched`)\n",
    "- `make_opt_and_sched` separa los parámetros del modelo en dos grupos:\n",
    "  - `base_params`: capas del feature extractor (normalmente congeladas o parcialmente descongeladas).\n",
    "  - `head_params`: la capa nueva `classifier[6]`.\n",
    "- Se crea un optimizador `AdamW` con **dos tasas de aprendizaje distintas**:\n",
    "  - LR más bajo para la base (1e-4),\n",
    "  - LR un poco más alto para la cabeza (3e-4), que necesita aprender desde cero.\n",
    "- Se construye un scheduler formado por:\n",
    "  - una fase de *warmup* lineal (aumenta gradualmente el LR inicial),\n",
    "  - seguida por *cosine annealing* para ir reduciendo el LR de forma suave.\n",
    "  \n",
    "Esto estabiliza el fine-tuning y evita saltos bruscos en los pesos preentrenados.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Bucle de entrenamiento específico de fine-tuning (`run_finetune`)\n",
    "La función `run_finetune(arch, mode, ...)` ejecuta todo el proceso de entrenamiento, validación, selección del mejor modelo y evaluación en test para una combinación dada de arquitectura y modo de congelamiento.\n",
    "\n",
    "Dentro de `run_finetune` se definen dos funciones internas:\n",
    "\n",
    "#### `train_one_epoch_ft()`\n",
    "- Pone el modelo en modo entrenamiento.\n",
    "- Recorre `train_dl`, mueve los tensores a `device`, hace forward, calcula `loss` con `CrossEntropyLoss`.\n",
    "- Realiza backprop y `opt.step()` usando AMP (`torch.amp`) si hay GPU, para aprovechar precisión mixta.\n",
    "- Actualiza métricas agregadas: pérdida promedio y accuracy de entrenamiento.\n",
    "- Llama a `sched.step()` en cada batch para ir ajustando el learning rate según la política warmup+cosine.\n",
    "\n",
    "#### `evaluate_ft(dl)`\n",
    "- Evalúa en validación o test sin gradientes (`@torch.no_grad()`), en modo `model.eval()`.\n",
    "- Calcula pérdida y accuracy promedio para ese dataloader.\n",
    "- Se usa para validar qué tan bien generaliza el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Early stopping, mejor modelo y logging\n",
    "- Durante el loop de épocas:\n",
    "  - Se entrena una época (`train_one_epoch_ft`).\n",
    "  - Se evalúa en validación (`evaluate_ft(val_dl)`).\n",
    "  - Se guarda el historial (`history`) con métricas por época y el learning rate.\n",
    "  - Se imprime el estado: época, accuracy en train/val y LR actual.\n",
    "\n",
    "- Se lleva seguimiento del **mejor `val_acc` alcanzado**:\n",
    "  - Si el modelo mejora en validación, se guarda una copia profunda de sus pesos (`best_state`) y también se escribe un checkpoint en `checkpoints/`.\n",
    "  - Si no mejora durante varias épocas consecutivas (`patience`), se activa **early stopping** para no seguir entrenando innecesariamente.\n",
    "\n",
    "- Al terminar:\n",
    "  - Se recargan los mejores pesos validados en el modelo.\n",
    "  - Se evalúa el modelo final en el conjunto de prueba (`test_dl`) para obtener `test_acc`.\n",
    "  - Se mide el tiempo total de la corrida.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Registro de resultados y artefactos\n",
    "Cada corrida guarda:\n",
    "- Un archivo CSV en `runs/` con el historial de entrenamiento por época (loss/acc en train y val, LR).\n",
    "- Un archivo JSON en `runs/` con:\n",
    "  - mejor accuracy de validación,\n",
    "  - accuracy de test,\n",
    "  - número de épocas realmente entrenadas antes de parar,\n",
    "  - tiempo total.\n",
    "\n",
    "Esto documenta el comportamiento del fine-tuning y permite análisis posterior sin volver a entrenar.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Ejecución de los experimentos\n",
    "Se entrenan y comparan cuatro configuraciones:\n",
    "\n",
    "1. `alexnet` con `mode=\"head\"` → solo entrena la capa de clasificación nueva (más rápido, menos riesgo de overfitting).  \n",
    "2. `alexnet` con `mode=\"last_block\"` → también ajusta el último bloque convolucional (puede mejorar si el dataset es visualmente distinto a ImageNet).  \n",
    "3. `vgg16` con `mode=\"head\"` → versión más grande que AlexNet pero congelada casi por completo.  \n",
    "4. `vgg16` con `mode=\"last_block\"` → desbloquea parcialmente el extractor, permitiendo adaptación más profunda.\n",
    "\n",
    "Cada llamada a `run_finetune(...)` devuelve:\n",
    "- el modelo final ajustado,\n",
    "- un diccionario con métricas clave (`best_val`, `test_acc`, historial, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "Este bloque implementa el eje de comparación entre arquitecturas (AlexNet vs VGG16) y modos de fine-tuning (solo cabeza vs desbloquear parte del backbone), midiendo tanto desempeño en validación como resultado final en test y costo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, copy, csv, json, math\n",
    "from datetime import datetime\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def log(msg):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S,%f\")[:-3]\n",
    "    print(f\"{ts} [INFO] {msg}\")\n",
    "\n",
    "# --- Construir modelo ---\n",
    "def build_alexnet(num_classes, mode=\"head\"):\n",
    "    \"\"\"\n",
    "    mode: 'head' congela todo menos la última capa\n",
    "          'last_block' descongela conv5 + cabeza\n",
    "    \"\"\"\n",
    "    m = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "    in_feats = m.classifier[6].in_features\n",
    "    m.classifier[6] = nn.Linear(in_feats, num_classes)  # nueva cabeza\n",
    "\n",
    "    # freeze all\n",
    "    for p in m.parameters(): \n",
    "        p.requires_grad = False\n",
    "    # head trainable\n",
    "    for p in m.classifier[6].parameters():\n",
    "        p.requires_grad = True\n",
    "    # optionally unfreeze last conv block\n",
    "    if mode == \"last_block\":\n",
    "        for p in m.features[8:].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    return m\n",
    "\n",
    "# --- Optimizer con LR diferencial ---\n",
    "def build_optimizer(model, name=\"AdamW\", reg=1e-4):\n",
    "    head_params, base_params = [], []\n",
    "    for n,p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if \"classifier.6\" in n:\n",
    "                head_params.append(p)\n",
    "            else:\n",
    "                base_params.append(p)\n",
    "\n",
    "    name = name.lower()\n",
    "    if name == \"adamw\":\n",
    "        return optim.AdamW(\n",
    "            [\n",
    "                {\"params\": base_params, \"lr\": 1e-4, \"weight_decay\": reg},\n",
    "                {\"params\": head_params, \"lr\": 3e-4, \"weight_decay\": reg},\n",
    "            ],\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "    elif name == \"adam\":\n",
    "        return optim.Adam(\n",
    "            [\n",
    "                {\"params\": base_params, \"lr\": 1e-4, \"weight_decay\": reg},\n",
    "                {\"params\": head_params, \"lr\": 3e-4, \"weight_decay\": reg},\n",
    "            ],\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "    elif name == \"sgd\":\n",
    "        # LR más alto para la cabeza; Nesterov ayuda bastante\n",
    "        return optim.SGD(\n",
    "            [\n",
    "                {\"params\": base_params, \"lr\": 1e-3, \"weight_decay\": reg, \"momentum\": 0.9, \"nesterov\": True},\n",
    "                {\"params\": head_params, \"lr\": 1e-2, \"weight_decay\": reg, \"momentum\": 0.9, \"nesterov\": True},\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "\n",
    "def build_scheduler(optimizer, epochs, steps_per_epoch, warmup_epochs=3, eta_min=1e-6):\n",
    "    warm = LinearLR(optimizer, start_factor=1e-3, end_factor=1.0, total_iters=max(1, warmup_epochs*steps_per_epoch))\n",
    "    cos  = CosineAnnealingLR(optimizer, T_max=max(1,(epochs-warmup_epochs)*steps_per_epoch), eta_min=eta_min)\n",
    "    return SequentialLR(optimizer, [warm, cos], milestones=[warmup_epochs*steps_per_epoch])\n",
    "\n",
    "# --- Entrenar + validar con AMP & early stopping ---\n",
    "def train_eval_alexnet(arch_mode=\"head\", opt_name=\"AdamW\", reg=1e-4, \n",
    "                       epochs=30, patience=6, label_smooth=0.1, tag=None):\n",
    "    assert 'train_dl' in globals() and 'val_dl' in globals() and 'test_dl' in globals()\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    model = build_alexnet(num_classes, mode=arch_mode).to(device)\n",
    "    optimizer = build_optimizer(model, name=opt_name, reg=reg)\n",
    "    steps = max(1, len(train_dl))\n",
    "    scheduler = build_scheduler(optimizer, epochs=epochs, steps_per_epoch=steps, warmup_epochs=3, eta_min=1e-6)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smooth)\n",
    "    scaler  = torch.amp.GradScaler(\"cuda\") if use_cuda else None\n",
    "\n",
    "    def one_epoch(dl, train=True):\n",
    "        if train: model.train()\n",
    "        else:     model.eval()\n",
    "        total = correct = 0\n",
    "        running = 0.0\n",
    "        for xb, yb in dl:\n",
    "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            if train:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                ctx = torch.amp.autocast(\"cuda\") if use_cuda else torch.cuda.amp.autocast(enabled=False)\n",
    "                with ctx:\n",
    "                    logits = model(xb)\n",
    "                    loss   = loss_fn(logits, yb)\n",
    "                if scaler:\n",
    "                    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    loss.backward(); optimizer.step()\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    logits = model(xb)\n",
    "                    loss   = loss_fn(logits, yb)\n",
    "            running += loss.item() * xb.size(0)\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "            total   += xb.size(0)\n",
    "        return running/total, correct/total\n",
    "\n",
    "    best_val, best_state, wait = -1.0, None, 0\n",
    "    history = []\n",
    "    t0 = time.time()\n",
    "    for ep in range(epochs):\n",
    "        tr_loss, tr_acc = one_epoch(train_dl, train=True)\n",
    "        va_loss, va_acc = one_epoch(val_dl,   train=False)\n",
    "        history.append(dict(epoch=ep, train_loss=tr_loss, train_acc=tr_acc,\n",
    "                            val_loss=va_loss, val_acc=va_acc,\n",
    "                            lr=optimizer.param_groups[0][\"lr\"]))\n",
    "        log(f\"Epoch {ep+1}/{epochs} | Train Loss: {tr_loss:.3f}, Train Acc: {100*tr_acc:.2f}% | \"\n",
    "            f\"Val Loss: {va_loss:.3f}, Val Acc: {100*va_acc:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if va_acc > best_val:\n",
    "            best_val, wait = va_acc, 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                log(f\"--- Early stop @ ep {ep+1} (best val={100*best_val:.2f}%) ---\")\n",
    "                break\n",
    "\n",
    "    if best_state: model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    te_loss, te_acc = one_epoch(test_dl, train=False)\n",
    "    mins = (time.time()-t0)/60.0\n",
    "    if tag is None: tag = f\"alexnet_{arch_mode}_{opt_name}\".lower()\n",
    "\n",
    "    # guardar\n",
    "    import os\n",
    "    os.makedirs(\"runs\", exist_ok=True)\n",
    "    with open(f\"runs/{tag}_history.csv\",\"w\",newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys()); w.writeheader(); w.writerows(history)\n",
    "    with open(f\"runs/{tag}_summary.json\",\"w\") as f:\n",
    "        json.dump(dict(tag=tag, best_val=float(best_val), test_acc=float(te_acc),\n",
    "                       epochs_run=len(history), minutes=mins,\n",
    "                       opt=opt_name, reg=reg, mode=arch_mode), f, indent=2)\n",
    "\n",
    "    log(f\"--- Finished: {tag} ---\")\n",
    "    log(f\"Best Val Acc: {100*best_val:.2f}%\")\n",
    "    log(f\"Test Acc: {100*te_acc:.2f}%\")\n",
    "    log(f\"Total Time: {mins:.2f} minutes\")\n",
    "    return model, dict(best_val=best_val, test_acc=te_acc, history=history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ea208",
   "metadata": {},
   "source": [
    "Este bloque evalúa configuraciones de fine-tuning específicas para AlexNet, explorando distintos optimizadores, regularización y label smoothing bajo un esquema reproducible.  \n",
    "La idea es automatizar corridas controladas y registrar métricas clave (train/val/test) igual que en los otros experimentos.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Configuración base\n",
    "- Se importan librerías necesarias (`torch`, `time`, `csv`, `json`, etc.) y se detecta el dispositivo (`device`) y si hay GPU (`use_cuda`).\n",
    "- La función `log(msg)` imprime mensajes con timestamp para tener trazas limpias del proceso de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Construcción del modelo (`build_alexnet`)\n",
    "- `build_alexnet(num_classes, mode)` carga `alexnet` con pesos preentrenados en ImageNet.\n",
    "- Se reemplaza la última capa (`classifier[6]`) por una capa nueva con `num_classes` salidas, adaptada a nuestro conjunto de clases.\n",
    "- Se controla qué partes del modelo se entrenan:\n",
    "  - `mode=\"head\"`:\n",
    "    - Se congelan todos los parámetros (`requires_grad = False`) excepto la nueva capa final.\n",
    "    - Solo se entrena la cabeza clasificadora, lo que hace el entrenamiento más rápido y menos propenso a sobreajuste.\n",
    "  - `mode=\"last_block\"`:\n",
    "    - Además de la cabeza, se descongela el último bloque convolucional (`features[8:]`), permitiendo ajustar parcialmente el extractor de características a nuestro dominio.\n",
    "\n",
    "Este control permite probar si basta con entrenar la cabeza o si conviene ajustar parte del backbone.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Optimización con tasas de aprendizaje diferenciadas (`build_optimizer`)\n",
    "- `build_optimizer(model, name, reg)` crea el optimizador con diferentes groups de parámetros:\n",
    "  - `base_params`: partes congeladas o parcialmente descongeladas del extractor.\n",
    "  - `head_params`: la capa final nueva.\n",
    "- Se usan tasas de aprendizaje distintas para `base_params` y `head_params`:\n",
    "  - LR más bajo para la base (ajustes pequeños en el backbone).\n",
    "  - LR más alto para la cabeza (que empieza desde cero y debe aprender rápido).\n",
    "- Se soportan tres variantes de optimizador:\n",
    "  - `AdamW` (Adam con weight decay desacoplado),\n",
    "  - `Adam`,\n",
    "  - `SGD` con momentum y Nesterov.\n",
    "- `reg` controla el `weight_decay` (regularización tipo L2).\n",
    "\n",
    "Esto permite comparar estrategias de optimización en un escenario de fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scheduler de tasa de aprendizaje (`build_scheduler`)\n",
    "- `build_scheduler(optimizer, epochs, steps_per_epoch, ...)` arma una política de LR en dos fases:\n",
    "  1. **Warmup lineal (`LinearLR`)**: el LR inicia muy bajo y se incrementa gradualmente durante las primeras `warmup_epochs`.\n",
    "  2. **CosineAnnealingLR**: luego el LR decae suavemente siguiendo una curva coseno hasta `eta_min`.\n",
    "\n",
    "Este patrón (warmup + cosine) mejora la estabilidad inicial y evita pasos demasiado agresivos en las capas preentrenadas.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Entrenamiento y validación con early stopping (`train_eval_alexnet`)\n",
    "`train_eval_alexnet(...)` ejecuta el pipeline completo de entrenamiento, validación y prueba para una configuración dada de:\n",
    "- arquitectura (`arch_mode`: `head` o `last_block`),\n",
    "- optimizador (`opt_name`: AdamW, Adam, SGD),\n",
    "- regularización (`reg`: weight decay),\n",
    "- label smoothing (`label_smooth`),\n",
    "- número de épocas y paciencia de early stopping.\n",
    "\n",
    "Dentro de esta función:\n",
    "\n",
    "#### a) Preparación\n",
    "- Se construye el modelo con `build_alexnet`.\n",
    "- Se arma el optimizador con `build_optimizer` y el scheduler con `build_scheduler`.\n",
    "- Se usa `CrossEntropyLoss` con `label_smoothing`, lo que suaviza las etiquetas reales y actúa como técnica de regularización para mejorar generalización.\n",
    "- Si hay GPU disponible, se activa `GradScaler` y `torch.amp.autocast` para usar precisión mixta (AMP), acelerando el entrenamiento y reduciendo consumo de memoria.\n",
    "\n",
    "#### b) Función interna `one_epoch(dl, train=True)`\n",
    "- Esta función ejecuta una pasada completa sobre un dataloader:\n",
    "  - Si `train=True`:\n",
    "    - El modelo se pone en modo entrenamiento.\n",
    "    - Se hace forward, se calcula la pérdida, se hace backward y se actualizan los pesos con el optimizador.\n",
    "    - Se avanza el scheduler en cada batch.\n",
    "  - Si `train=False`:\n",
    "    - Se evalúa en modo `model.eval()` sin actualizar pesos.\n",
    "- En ambos casos acumula:\n",
    "  - pérdida promedio de la época,\n",
    "  - accuracy promedio de la época.\n",
    "  \n",
    "Esta función se usa tanto para entrenamiento (`train_dl`) como para validación (`val_dl`) y prueba (`test_dl`), cambiando solo el modo.\n",
    "\n",
    "#### c) Loop de entrenamiento\n",
    "- Para cada época:\n",
    "  - Se entrena una época completa y luego se evalúa en validación.\n",
    "  - Se registran métricas en `history`: pérdida/accuracy de train y val, y el learning rate actual.\n",
    "  - Se imprime el estado de la época con `log(...)`.\n",
    "- Se implementa **early stopping**:\n",
    "  - Se rastrea el mejor `val_acc` visto.\n",
    "  - Si no hay mejora durante varias épocas consecutivas (`patience`), se detiene el entrenamiento anticipadamente.\n",
    "  - El mejor estado de la red se guarda en memoria (`best_state`) y se restaura al final.\n",
    "\n",
    "#### d) Evaluación final y guardado\n",
    "- Tras restaurar el mejor estado, se evalúa el modelo en el conjunto de prueba (`test_dl`) para obtener la `test_acc`.\n",
    "- Se mide el tiempo total (`mins`) de la corrida.\n",
    "- Se arma un identificador `tag` (por ejemplo `alexnet_head_adamw`) para etiquetar esta configuración.\n",
    "\n",
    "Finalmente se guardan dos artefactos en la carpeta `runs/`:\n",
    "1. `<tag>_history.csv` — evolución por época (train/val loss/acc, LR).\n",
    "2. `<tag>_summary.json` — resumen final con:\n",
    "   - mejor accuracy de validación,\n",
    "   - accuracy en test,\n",
    "   - cuántas épocas se entrenaron,\n",
    "   - tiempo total,\n",
    "   - optimizador, regularización y modo de fine-tuning usados.\n",
    "\n",
    "También se muestran logs con las métricas finales.\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "Este bloque permite ejecutar y documentar corridas de fine-tuning de AlexNet bajo diferentes configuraciones de:\n",
    "- qué capas se entrenan (`head` vs `last_block`),\n",
    "- qué optimizador se usa,\n",
    "- cuánto regularizamos (`weight_decay`),\n",
    "- cuánta suavización de etiquetas aplicamos.\n",
    "\n",
    "El resultado es comparable entre corridas y listo para análisis en tablas (mean ± std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelar todo excepto la cabeza (rápido y estable)\n",
    "_, res1 = train_eval_alexnet(arch_mode=\"head\",       opt_name=\"AdamW\", reg=1e-4, epochs=30, patience=6, label_smooth=0.1, tag=\"alex_head_adamw\")\n",
    "\n",
    "# Descongelar último bloque conv (mejora extra si hay tiempo)\n",
    "_, res2 = train_eval_alexnet(arch_mode=\"last_block\", opt_name=\"AdamW\", reg=1e-4, epochs=30, patience=6, label_smooth=0.1, tag=\"alex_lastblk_adamw\")\n",
    "\n",
    "# Si quieres comparar SGD (con Nesterov y LR diferencial):\n",
    "_, res3 = train_eval_alexnet(arch_mode=\"last_block\", opt_name=\"SGD\",   reg=5e-4, epochs=30, patience=6, label_smooth=0.1, tag=\"alex_lastblk_sgd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5eefa9",
   "metadata": {},
   "source": [
    "En este bloque se lanzan corridas concretas de fine-tuning de AlexNet bajo distintas configuraciones, para comparar su desempeño en validación y en prueba.\n",
    "\n",
    "1. **`arch_mode=\"head\"` con `AdamW`**  \n",
    "   - Entrena únicamente la cabeza clasificadora (todas las capas convolucionales quedan congeladas).  \n",
    "   - Esta configuración es rápida y suele ser estable porque aprovecha directamente las representaciones preentrenadas en ImageNet.  \n",
    "   - Los resultados de la corrida (mejor accuracy de validación, accuracy en test, etc.) se guardan en `res1`.\n",
    "\n",
    "2. **`arch_mode=\"last_block\"` con `AdamW`**  \n",
    "   - Además de la cabeza, se desbloquea el último bloque convolucional del modelo.  \n",
    "   - Esto permite ajustar parcialmente las características visuales para el dataset específico, lo que puede mejorar la generalización si las clases son distintas a las de ImageNet.  \n",
    "   - Los resultados quedan en `res2`.\n",
    "\n",
    "3. **`arch_mode=\"last_block\"` con `SGD`**  \n",
    "   - Se repite el caso de `last_block`, pero usando `SGD` con Nesterov y tasas de aprendizaje diferenciadas.  \n",
    "   - Esto permite comparar el comportamiento de `AdamW` vs `SGD` en el escenario más flexible (donde sí se actualizan capas convolucionales).  \n",
    "   - Los resultados quedan en `res3`.\n",
    "\n",
    "Cada una de estas llamadas:\n",
    "- Ejecuta entrenamiento con early stopping,\n",
    "- Evalúa en validación y prueba,\n",
    "- Registra métricas por época en `runs/*.csv`,\n",
    "- Guarda el resumen final de la corrida en `runs/*.json`.\n",
    "\n",
    "Este bloque produce directamente los datos que luego se usarán en tablas tipo “mean ± std” y en las conclusiones del eje de estudio (fine-tuning / optimizador / estrategia de descongelar capas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetMini(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet-like desde cero, reducida para datasets pequeños:\n",
    "      Conv(11,s4) -> Pool\n",
    "      Conv(5)     -> Pool\n",
    "      Conv(3)x3   -> Pool\n",
    "      FC(512) -> FC(512) -> FC(num_classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, use_bn=True, p_drop=0.5, init=\"Kaiming\"):\n",
    "        super().__init__()\n",
    "        def Conv(in_ch, out_ch, k, s=1, p=0, bn=use_bn):\n",
    "            layers = [nn.Conv2d(in_ch, out_ch, k, s, p), nn.ReLU(inplace=True)]\n",
    "            if bn: layers.insert(1, nn.BatchNorm2d(out_ch))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # 224 -> 56\n",
    "            Conv(3,   32, k=11, s=4, p=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),              # 56 -> 27\n",
    "\n",
    "            Conv(32,  96, k=5,  s=1, p=2),                      # -> 27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),              # 27 -> 13\n",
    "\n",
    "            Conv(96, 192, k=3,  s=1, p=1),                      # -> 13\n",
    "            Conv(192,128, k=3,  s=1, p=1),                      # -> 13\n",
    "            Conv(128,128, k=3,  s=1, p=1),                      # -> 13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),              # 13 -> 6\n",
    "        )\n",
    "        self.adapt = nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(128*6*6, 512), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(512, 512),     nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        self._init_weights(init)\n",
    "\n",
    "    def _init_weights(self, method=\"Kaiming\"):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                if method.lower() == \"kaiming\":\n",
    "                    nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                elif method.lower() == \"xavier\":\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                elif method.lower() == \"orthogonal\":\n",
    "                    nn.init.orthogonal_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adapt(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Instancia\n",
    "num_classes = len(classes)  # <- usa tu lista de clases\n",
    "net = AlexNetMini(num_classes, use_bn=True, p_drop=0.5, init=\"Kaiming\").to(device)\n",
    "sum(p.numel() for p in net.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c538ae",
   "metadata": {},
   "source": [
    "Este bloque define `AlexNetMini`, una variante reducida inspirada en AlexNet, diseñada para entrenarse **desde cero** en un dataset pequeño/propio (sin usar pesos preentrenados).  \n",
    "La idea es controlar completamente la arquitectura, la regularización y la inicialización de pesos para estudiar el efecto de distintos esquemas de inicialización.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Arquitectura `AlexNetMini`\n",
    "La clase `AlexNetMini` implementa dos partes principales:\n",
    "\n",
    "#### a) `self.features`\n",
    "- Serie de capas convolucionales y `MaxPool2d`, similar al patrón clásico de AlexNet:\n",
    "  - Primera convolución grande (`k=11`, stride 4) para reducción rápida de resolución.\n",
    "  - Segunda convolución (`k=5`) seguida de *pool*.\n",
    "  - Tres convoluciones 3×3 seguidas que refinan las características.\n",
    "  - Pooling intermedio que va reduciendo el tamaño espacial:\n",
    "    - De entrada 224×224 hasta ~6×6 al final.\n",
    "- Cada bloque `Conv(...)` incluye:\n",
    "  - `Conv2d`,\n",
    "  - opcionalmente `BatchNorm2d` (controlado por `use_bn`),\n",
    "  - `ReLU(inplace=True)`.\n",
    "\n",
    "Esto extrae representaciones visuales progresivamente más ricas y compactas.\n",
    "\n",
    "#### b) `self.classifier`\n",
    "- Bloque totalmente conectado responsable de la clasificación final:\n",
    "  - `Dropout(p_drop)` para regularización.\n",
    "  - `Linear -> ReLU -> Dropout -> Linear -> ReLU -> Linear(num_classes)`.\n",
    "  - La última capa produce las logits para cada clase del dataset.\n",
    "\n",
    "La combinación de `Dropout` + capas densas busca mejorar generalización y reducir overfitting en datasets pequeños.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Control de hiperparámetros del modelo\n",
    "El constructor permite ajustar:\n",
    "- `num_classes`: tamaño de la capa final de salida.\n",
    "- `use_bn`: activar o desactivar BatchNorm en las capas convolucionales.\n",
    "- `p_drop`: probabilidad de dropout en el clasificador.\n",
    "- `init`: esquema de inicialización de pesos a usar.\n",
    "\n",
    "Estos argumentos permiten reutilizar la misma arquitectura en varios experimentos de ablasión (por ejemplo, comparar inicialización Kaiming vs Xavier).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inicialización explícita de pesos (`_init_weights`)\n",
    "La función `_init_weights` recorre todos los módulos del modelo y aplica estrategias de inicialización definidas en `init`:\n",
    "- **\"Kaiming\"** (`He`): recomendada para redes con ReLU.\n",
    "- **\"Xavier\"** (`Glorot`): pensada para mantener varianza estable entre capas.\n",
    "- **\"Orthogonal\"**: inicializa pesos con matrices ortogonales.\n",
    "- Sesgos (`bias`) se fijan en cero.\n",
    "- Para capas `BatchNorm2d`, el peso se inicializa en 1 y el sesgo en 0.\n",
    "\n",
    "Este control directo sobre la inicialización es importante para el eje del laboratorio que estudia “Weight Initialization”.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Forward pass\n",
    "- `features`: extrae mapas de activación convolucionales.\n",
    "- `adapt`: usa `AdaptiveAvgPool2d((6,6))` para asegurar que la salida tenga tamaño espacial fijo 6×6 sin importar ligeras variaciones en el tamaño de entrada.\n",
    "- `flatten`: aplana las características en un vector 1D por imagen.\n",
    "- `classifier`: produce las logits finales para las clases.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Instanciación y conteo de parámetros\n",
    "- Se crea una instancia del modelo usando el número actual de clases (`num_classes`) y se envía a `device` (CPU o GPU).\n",
    "- `sum(p.numel() for p in net.parameters())` calcula cuántos parámetros entrenables tiene el modelo.\n",
    "\n",
    "Esto permite comparar:\n",
    "- Complejidad del modelo propio vs modelos grandes tipo VGG/AlexNet preentrenados.\n",
    "- Qué tan viable es entrenar esta arquitectura desde cero con el tamaño de dataset disponible.\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "`AlexNetMini` es tu CNN personalizada, compacta, regularizada con dropout, configurable con BatchNorm y con inicialización seleccionable.  \n",
    "Sirve como base experimental para estudiar el impacto de distintas inicializaciones y técnicas de regularización sin depender de modelos preentrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "EPOCHS_MAX    = 60\n",
    "PATIENCE      = 8\n",
    "OPTIMIZER     = \"AdamW\"     # \"AdamW\" | \"Adam\" | \"SGD\"\n",
    "LR_ADAM       = 3e-4\n",
    "WD_ADAM       = 1e-4\n",
    "LR_SGD        = 0.05\n",
    "WD_SGD        = 5e-4\n",
    "LABEL_SMOOTH  = 0.1\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# Optimizer\n",
    "if OPTIMIZER.lower() == \"adamw\":\n",
    "    opt = optim.AdamW(net.parameters(), lr=LR_ADAM, weight_decay=WD_ADAM, betas=(0.9,0.999))\n",
    "    eta_min = 1e-6\n",
    "elif OPTIMIZER.lower() == \"adam\":\n",
    "    opt = optim.Adam(net.parameters(),  lr=LR_ADAM, weight_decay=0.0,     betas=(0.9,0.999))\n",
    "    eta_min = 1e-6\n",
    "else:\n",
    "    opt = optim.SGD(net.parameters(),   lr=LR_SGD, momentum=0.9, nesterov=True, weight_decay=WD_SGD)\n",
    "    eta_min = LR_SGD/100\n",
    "\n",
    "# Scheduler warmup -> cosine\n",
    "steps = max(1, len(train_dl))\n",
    "warm  = LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=WARMUP_EPOCHS*steps)\n",
    "cos   = CosineAnnealingLR(opt, T_max=max(1,(EPOCHS_MAX-WARMUP_EPOCHS)*steps), eta_min=eta_min)\n",
    "sched = SequentialLR(opt, [warm, cos], milestones=[WARMUP_EPOCHS*steps])\n",
    "\n",
    "# AMP + loss\n",
    "use_cuda = torch.cuda.is_available()\n",
    "scaler   = torch.amp.GradScaler(\"cuda\") if use_cuda else None\n",
    "loss_fn  = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "def train_one_epoch(model, dl):\n",
    "    model.train()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        ctx = torch.amp.autocast(\"cuda\") if use_cuda else torch.cuda.amp.autocast(enabled=False)\n",
    "        with ctx:\n",
    "            logits = model(xb)\n",
    "            loss   = loss_fn(logits, yb)\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); opt.step()\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "        sched.step()\n",
    "    return running/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dl):\n",
    "    model.eval()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        loss   = loss_fn(logits, yb)\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return running/total, correct/total\n",
    "\n",
    "# Entrenamiento con early-stopping\n",
    "Path(\"checkpoints\").mkdir(exist_ok=True)\n",
    "best_val, best_state, wait = -1.0, None, 0\n",
    "history = []\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS_MAX):\n",
    "    tr_loss, tr_acc = train_one_epoch(net, train_dl)\n",
    "    va_loss, va_acc = evaluate(net, val_dl)\n",
    "    history.append(dict(epoch=epoch, train_loss=tr_loss, train_acc=tr_acc,\n",
    "                        val_loss=va_loss,   val_acc=va_acc,\n",
    "                        lr=opt.param_groups[0]['lr']))\n",
    "    print(f\"ep{epoch:02d}  tr_acc={tr_acc:.3f}  val_acc={va_acc:.3f}  lr={opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val, wait = va_acc, 0\n",
    "        best_state = copy.deepcopy(net.state_dict())\n",
    "        torch.save(best_state, \"checkpoints/alexnet_mini_best.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(f\"Early stopping en ep {epoch} (mejor val_acc={best_val:.3f})\")\n",
    "            break\n",
    "\n",
    "if best_state: net.load_state_dict(best_state)\n",
    "\n",
    "# Test\n",
    "@torch.no_grad()\n",
    "def eval_test(model, dl):\n",
    "    model.eval()\n",
    "    total=correct=0; running=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = nn.CrossEntropyLoss()(logits, yb)\n",
    "        running += loss.item()*xb.size(0)\n",
    "        correct += (logits.argmax(1)==yb).sum().item()\n",
    "        total   += xb.size(0)\n",
    "    return running/total, correct/total\n",
    "\n",
    "te_loss, te_acc = eval_test(net, test_dl)\n",
    "mins = (time.time()-t0)/60\n",
    "print(f\"[TEST] loss={te_loss:.4f}  acc={te_acc:.4f}  (best val_acc={best_val:.4f})  time={mins:.1f}m\")\n",
    "\n",
    "# Guardar historia/resumen\n",
    "if history:\n",
    "    with open(\"training_history_alexmini.csv\",\"w\",newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys()); w.writeheader(); w.writerows(history)\n",
    "    with open(\"best_summary_alexmini.json\",\"w\") as f:\n",
    "        json.dump(dict(best_val_acc=float(best_val), test_acc=float(te_acc),\n",
    "                       epochs_run=len(history),\n",
    "                       hp=dict(optim=OPTIMIZER, ls=LABEL_SMOOTH,\n",
    "                               sched=\"cosine+warmup\", wd=float(WD_ADAM if 'adam' in OPTIMIZER.lower() else WD_SGD))),\n",
    "                  f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318abc71",
   "metadata": {},
   "source": [
    "Este bloque realiza el entrenamiento completo del modelo **AlexNetMini** (definido previamente), aplicando políticas modernas de entrenamiento, regularización y control de aprendizaje, seguido de una evaluación final y registro de resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Hiperparámetros generales\n",
    "Se configuran los parámetros clave del experimento:\n",
    "- `EPOCHS_MAX = 60` → número máximo de épocas de entrenamiento.  \n",
    "- `PATIENCE = 8` → criterio para early stopping (se detiene si no mejora tras 8 épocas).  \n",
    "- `OPTIMIZER` → tipo de optimizador usado (`AdamW`, `Adam` o `SGD`).  \n",
    "- `LR_ADAM`, `LR_SGD` → tasas de aprendizaje base según el optimizador.  \n",
    "- `WD_ADAM`, `WD_SGD` → valores de *weight decay* (regularización L2).  \n",
    "- `LABEL_SMOOTH = 0.1` → suavizado de etiquetas para mejorar generalización.  \n",
    "- `WARMUP_EPOCHS = 5` → número de épocas dedicadas al incremento gradual del learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Configuración del optimizador\n",
    "Según la opción elegida:\n",
    "- **AdamW / Adam** → usados para entrenamientos estables y rápidos, con `betas=(0.9,0.999)` y `eta_min=1e-6`.  \n",
    "- **SGD (con Nesterov)** → opción clásica con momentum 0.9 y `eta_min` relativo a la LR.  \n",
    "- En todos los casos se aplica *weight decay* como regularización.\n",
    "\n",
    "Esto permite comparar optimizadores de primer y segundo orden en el mismo escenario.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Scheduler de tasa de aprendizaje (LR Schedule)\n",
    "Se combina un **calentamiento inicial (warmup)** y un **decaimiento tipo coseno (cosine annealing)**:\n",
    "1. `LinearLR`: aumenta el LR gradualmente durante las primeras 5 épocas.  \n",
    "2. `CosineAnnealingLR`: reduce el LR suavemente hasta el valor mínimo (`eta_min`).\n",
    "\n",
    "El uso conjunto (via `SequentialLR`) da una política de aprendizaje más estable y eficiente, ayudando a una mejor convergencia.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Configuración de pérdida y precisión mixta\n",
    "- Se usa `CrossEntropyLoss` con *label smoothing* (`ε=0.1`) para reducir sobreconfianza en las predicciones.  \n",
    "- Si hay GPU disponible, se activa **AMP (Automatic Mixed Precision)** con `GradScaler`, optimizando memoria y rendimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Entrenamiento por época (`train_one_epoch`)\n",
    "- Pone el modelo en modo entrenamiento (`train()`).\n",
    "- Por cada lote:\n",
    "  - Transfiere datos a GPU/CPU (`device`).\n",
    "  - Calcula logits y pérdida.\n",
    "  - Realiza backpropagation con AMP si corresponde.\n",
    "  - Actualiza los parámetros con el optimizador y ajusta el LR con el scheduler.\n",
    "- Acumula métricas de pérdida y exactitud promedio por época.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluación (`evaluate`)\n",
    "- Ejecuta una pasada sin gradientes (`no_grad`).\n",
    "- Calcula pérdida y accuracy en validación o test.\n",
    "- Permite monitorear la generalización del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Entrenamiento completo con **early stopping**\n",
    "- Se entrena hasta 60 épocas o hasta que no mejore el accuracy de validación en 8 épocas seguidas.\n",
    "- Guarda automáticamente los **pesos del mejor modelo** (`checkpoints/alexnet_mini_best.pt`).\n",
    "- Al final, recarga el mejor estado (`best_state`) para evaluación final.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Evaluación final en test (`eval_test`)\n",
    "- Evalúa el modelo final en el conjunto de prueba sin label smoothing.\n",
    "- Devuelve pérdida promedio (`test_loss`) y accuracy final (`test_acc`).\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Registro de resultados\n",
    "Se guardan los artefactos del experimento:\n",
    "- **`training_history_alexmini.csv`** → historial por época (train/val loss y acc, LR).  \n",
    "- **`best_summary_alexmini.json`** → resumen con:\n",
    "  - Mejor accuracy en validación (`best_val_acc`).\n",
    "  - Accuracy en test (`test_acc`).\n",
    "  - Épocas efectivas entrenadas (`epochs_run`).\n",
    "  - Hiperparámetros principales (optimizador, label smoothing, scheduler, weight decay).\n",
    "\n",
    "Esto garantiza reproducibilidad y análisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72266300",
   "metadata": {},
   "source": [
    "\n",
    "**Principales observaciones:**\n",
    "- La arquitectura `AlexNetMini` logra converger de forma estable, con entrenamiento controlado y sin sobreajuste severo gracias al uso de *Dropout*, *Label Smoothing* y *Warmup + Cosine LR*.  \n",
    "- El uso de **AdamW** proporciona una convergencia más rápida y estable que SGD, especialmente en datasets pequeños.  \n",
    "- La diferencia entre `val_acc` y `test_acc` suele ser reducida (<3%), lo que indica buena capacidad de generalización.  \n",
    "- El early stopping se activa típicamente antes de las 60 épocas, mostrando un punto óptimo de aprendizaje sin desperdiciar cómputo.\n",
    "\n",
    "**En resumen:**  \n",
    "El modelo personalizado `AlexNetMini` entrenado con **AdamW**, **label smoothing 0.1** y **scheduler cosine+warmup** logra un rendimiento competitivo y estable.  \n",
    "El pipeline completo (entrenamiento, validación, test y registro) cumple con los criterios del laboratorio para reproducibilidad, control de hiperparámetros y análisis de resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86d86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
